{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Intro to Machine Learning (feat. Tensorflow)\n",
    "\n",
    "2017.4.13\n",
    "Dongwoo 화목회"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Machine learning is the subfield of computer science that gives computers the ability to learn without being explicitly programmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Supervised vs Unsupervised ( vs reinforcement )\n",
    "- supervised\n",
    "    - Regression problem\n",
    "    - Classification problem\n",
    "- unsupervised\n",
    "    - Clustering\n",
    "- reinforecement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Tensorflow\n",
    "Tensorflow is an open-source software library for Machine Intelligence. It was developed by Google to meet their needs for systems capable of building and training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Before Start.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hypothesis\n",
    "H(x) = Wx + b에서 Wx + b는 x에 대한 1차 방적식으로 직선을 표현한다. 기울기에 해당하는 W(Weight)와 절편에 해당하는 b(bias)가 반복되는 과정에서 계속 바뀌고, 마지막 루프에서 바뀐 최종 값을 사용해서 데이터 예측(prediction)에 사용하게 된다. 최종 결과로 나온 가설을 모델(model)이라고 부르고, \"학습되었다\"라고 한다.\n",
    "![title](hypothesis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Cost function\n",
    "앞에서 설명한 Hypothesis 방정식에 대한 비용(cost)으로 방정식의 결과가 크게 나오면 좋지 않다고 얘기하고, 루프를 돌 때마다 W와 b를 비용이 적게 발생하는 방향으로 수정하게 된다.\n",
    "![title](cost_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Goal : Minimize cost\n",
    "목표는 cost를 최소로 만드는 W(기울기)와 b(절편)를 찾는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Linear Regression\n",
    "2차원 좌표에 분포된 데이터를 1차원 직선 방정식을 통해 표현되지 않은 데이터를 예측하기 위한 분석 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Gradient descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![title](linear1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![title](linear2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 164.13 [-6.31108856] [ 29.59743118]\n",
      "200 0.000739635 [ 1.97736549] [ 0.06654793]\n",
      "400 3.87456e-09 [ 1.99994826] [ 0.00015244]\n",
      "600 1.13687e-13 [ 1.99999988] [  5.47490401e-07]\n",
      "800 0.0 [ 2.] [  1.18337212e-07]\n",
      "1000 0.0 [ 2.] [  1.18337212e-07]\n",
      "1200 0.0 [ 2.] [  1.18337212e-07]\n",
      "1400 0.0 [ 2.] [  1.18337212e-07]\n",
      "1600 0.0 [ 2.] [  1.18337212e-07]\n",
      "1800 0.0 [ 2.] [  1.18337212e-07]\n",
      "2000 0.0 [ 2.] [  1.18337212e-07]\n",
      "[ 10.]\n",
      "[ 5.]\n"
     ]
    }
   ],
   "source": [
    "x_data = [1., 2., 3., 4.]\n",
    "y_data = [2., 4., 6., 8.]\n",
    "\n",
    "# range is -100 ~ 100\n",
    "W = tf.Variable(tf.random_uniform([1], -100., 100.))\n",
    "b = tf.Variable(tf.random_uniform([1], -100., 100.))\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "hypothesis = W * X + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "rate = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(2001):\n",
    "    sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 200 == 0:\n",
    "        print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W), sess.run(b))\n",
    "\n",
    "print(sess.run(hypothesis, feed_dict={X: 5}))           # [ 10.]\n",
    "print(sess.run(hypothesis, feed_dict={X: 2.5}))         # [5.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Logistic Classification\n",
    "Linear Regression을 활용해서 데이터를 분류하는 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Sigmoid\n",
    "sigmoid는 linear regression에서 가져온 값을 0과 1 사이의 값으로 변환한다. x가 0일 때, 0.5가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![title](sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### New cost function\n",
    "sigmoid 함수 적용으로 인해 새로운 cost function를 사용해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![title](cost_function2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Gradient Decent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![title](gradient.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.46044 [[-0.74296248 -0.2775833  -0.55979639]]\n",
      "200 0.45272 [[-2.1767118   0.19811228  0.44593912]]\n",
      "400 0.403735 [[-3.14632273  0.28867716  0.58383071]]\n",
      "600 0.379799 [[-3.82585311  0.34539157  0.68563265]]\n",
      "800 0.36602 [[-4.34201002  0.38395023  0.76612681]]\n",
      "1000 0.357206 [[-4.75503063  0.41178772  0.83253658]]\n",
      "1200 0.351144 [[-5.09761953  0.43277982  0.88896227]]\n",
      "1400 0.346749 [[-5.38932276  0.44914049  0.93794906]]\n",
      "1600 0.343434 [[-5.64268684  0.46222582  0.98118448]]\n",
      "1800 0.340853 [[-5.86621332  0.47291279  1.01984453]]\n",
      "2000 0.338793 [[-6.06590843  0.48179361  1.05477953]]\n",
      "-----------------------------------------\n",
      "[1, 2, 2] : [[False]]\n",
      "[1, 5, 5] : [[ True]]\n"
     ]
    }
   ],
   "source": [
    "# 04train.txt\n",
    "# #x0 x1 x2 y\n",
    "# 1   2   1   0\n",
    "# 1   3   2   0\n",
    "# 1   3   5   0\n",
    "# 1   5   5   1\n",
    "# 1   7   5   1\n",
    "# 1   2   5   1\n",
    "\n",
    "# 원본 파일은 6행 4열이지만, 열 우선이라서 4행 6열로 가져옴\n",
    "xy = np.loadtxt('04train.txt', unpack=True, dtype='float32')\n",
    "\n",
    "# print(xy[0], xy[-1])        # [ 1.  1.  1.  1.  1.  1.] [ 0.  0.  0.  1.  1.  1.]\n",
    "\n",
    "x_data = xy[:-1]            # 3행 6열\n",
    "y_data = xy[-1]             # 1행 6열\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# feature별 가중치를 난수로 초기화. feature는 bias 포함해서 3개. 1행 3열.\n",
    "W = tf.Variable(tf.random_uniform([1, len(x_data)], -1.0, 1.0))\n",
    "\n",
    "# 행렬 곱셈. (1x3) * (3x6)\n",
    "h = tf.matmul(W, X)\n",
    "hypothesis = tf.div(1., 1. + tf.exp(-h))    # exp(-h) = e ** -h. e는 자연상수\n",
    "\n",
    "# exp()에는 실수만 전달\n",
    "# print(tf.exp([1., 2., 3.]).eval())      # [2.71828175 7.38905621 20.08553696]\n",
    "# print(tf.exp([-1., -2., -3.]).eval())   # [0.36787945 0.13533528 0.04978707]\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "\n",
    "rate = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(2001):\n",
    "    sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 200 == 0:\n",
    "        print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    "\n",
    "print('-----------------------------------------')\n",
    "\n",
    "# 결과가 0 또는 1로 계산되는 것이 아니라 0과 1 사이의 값으로 나오기 때문에 True/False는 직접 판단\n",
    "print('[1, 2, 2] :', sess.run(hypothesis, feed_dict={X: [[1], [2], [2]]}) > 0.5)\n",
    "print('[1, 5, 5] :', sess.run(hypothesis, feed_dict={X: [[1], [5], [5]]}) > 0.5)\n",
    "# print('[1, 4, 2] [1, 0, 10] :', end=' ')\n",
    "# print(sess.run(hypothesis, feed_dict={X: [[1, 1], [4, 0], [2, 10]]}) > 0.5)\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Softmax Regression\n",
    "Logistic Regression을 부르는 다른 이름은 binary classification이다. 데이터를 1과 0의 두 가지 그룹으로 나누기 위해 사용하는 모델이다. Softmax는 데이터를 2개 이상의 그룹으로 나누기 위해 binary classification을 확장한 모델이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![title](softmax1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Softmax\n",
    "softmax는 점수로 나온 결과를 전체 합계가 1이 되는 0과 1 사이의 값으로 변경해 준다. 전체를 더하면 1이 되기 때문에 확률(probabilites)이라고 부르면 의미가 더욱 분명해진다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![title](softmax2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-Hot Encoding\n",
    "one-hot encoding은 softmax로 구한 값 중에서 가장 큰 값을 1로, 나머지를 0으로 만든다. 어떤 것을 선택할지를 확실하게 정리해 준다. one-hot encoding은 설명한 것처럼 매우 간단하기 때문에 직접 구현할 수도 있지만, 텐서플로우에서는 argmax 함수라는 이름으로 제공하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![title](softmax3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Cross-Entropy Cost Function\n",
    "S(Y)는 softmax가 예측한 값이고, L(Y)는 실제 Y의 값으로 L은 label을 의미한다. cost 함수는 예측한 값과 실제 값의 거리(distance, D)를 계산하는 함수로, 이 값이 줄어드는 방향으로, 즉 entropy가 감소하는 방향으로 진행하다 보면 최저점을 만나게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![title](softmax4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![title](softmax5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data : (8, 3)\n",
      "y_data : (8, 3)\n",
      "   0  1.09048 [-0.00083333  0.00041667  0.00041667] [ 0.00166667  0.00291667 -0.00458333] [ 0.00166667  0.00416667 -0.00583333]\n",
      " 200 0.985653 [-0.21679303 -0.05050437  0.26729742] [ 0.02901031 -0.06265054  0.03364029] [ 0.04244109  0.12451769 -0.16695869]\n",
      " 400 0.926073 [-0.41495511 -0.10318027  0.51813519] [ 0.03762176 -0.10302337  0.06540174] [ 0.07457665  0.17761268 -0.25218898]\n",
      " 600 0.879342 [-0.59596533 -0.1515553   0.74752003] [ 0.04480708 -0.11983915  0.07503238] [ 0.10447746  0.20644082 -0.31091776]\n",
      " 800 0.840971 [-0.76270223 -0.19499816  0.95770001] [ 0.05223157 -0.12450957  0.07227841] [ 0.13095778  0.22218271 -0.35314   ]\n",
      "1000 0.808647 [-0.91752577 -0.2334879   1.15101326] [ 0.06007884 -0.12292791  0.06284945] [ 0.15438823  0.23068959 -0.38507724]\n",
      "1200 0.780959 [-1.06231129 -0.26727253  1.32958329] [ 0.06808005 -0.11823834  0.05015875] [ 0.17550454  0.23514733 -0.41065112]\n",
      "1400 0.756943 [-1.19854808 -0.29670808  1.49525583] [ 0.07591439 -0.11214777  0.03623381] [ 0.19498996  0.237331   -0.43232018]\n",
      "1600 0.735893 [-1.32743537 -0.32218221  1.64961684] [ 0.08333746 -0.10557999  0.022243  ] [ 0.21336642  0.23823628 -0.45160189]\n",
      "1800 0.717269 [-1.44994974 -0.34407791  1.79402602] [ 0.09020081 -0.09902246  0.00882213] [ 0.23099625  0.23841871 -0.46941414]\n",
      "2000 0.700649 [-1.56689739 -0.36275655  1.92965221] [ 0.09643649 -0.09271803 -0.00371792] [ 0.24811605  0.23818412 -0.48629922]\n",
      "-------------------------------\n",
      "a : [[ 0.68849677  0.26731515  0.04418808]] [0]\n",
      "b : [[ 0.24322268  0.44183081  0.3149465 ]] [1]\n",
      "c : [[ 0.02974809  0.08208466  0.8881672 ]] [2]\n",
      "d :  [ 0.68849677  0.26731515  0.04418808] [ 0.24322268  0.44183081  0.3149465 ] [ 0.02974809  0.08208466  0.8881672 ] [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# softmax이기 때문에 y를 표현할 때, 벡터로 표현한다.\n",
    "# 1개의 값으로 표현한다고 할 때, 뭐라고 쓸지도 사실 애매하다.\n",
    "\n",
    "# 05train.txt\n",
    "# #x0 x1 x2 y[A   B   C]\n",
    "# 1   2   1   0   0   1     # C\n",
    "# 1   3   2   0   0   1\n",
    "# 1   3   4   0   0   1\n",
    "# 1   5   5   0   1   0     # B\n",
    "# 1   7   5   0   1   0\n",
    "# 1   2   5   0   1   0\n",
    "# 1   6   6   1   0   0     # A\n",
    "# 1   7   7   1   0   0\n",
    "\n",
    "xy = np.loadtxt('05train.txt', unpack=True, dtype='float32')\n",
    "\n",
    "# xy는 6x8. xy[:3]은 3x8. 행렬 곱셈을 하기 위해 미리 transpose.\n",
    "x_data = np.transpose(xy[:3])\n",
    "y_data = np.transpose(xy[3:])\n",
    "\n",
    "print('x_data :', x_data.shape)     # x_data : (8, 3)\n",
    "print('y_data :', y_data.shape)     # y_data : (8, 3)\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])  # x_data와 같은 크기의 열 가짐. 행 크기는 모름.\n",
    "Y = tf.placeholder(\"float\", [None, 3])  # tf.float32라고 써도 됨\n",
    "\n",
    "W = tf.Variable(tf.zeros([3, 3]))       # 3x3 행렬. 전체 0.\n",
    "\n",
    "# softmax 알고리듬 적용. X*W = (8x3) * (3x3) = (8x3)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W))\n",
    "\n",
    "# cross-entropy cost 함수\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), reduction_indices=1))\n",
    "\n",
    "learning_rate = 0.01\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(2001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            feed = {X: x_data, Y: y_data}\n",
    "            print('{:4} {:8.6}'.format(step, sess.run(cost, feed_dict=feed)), *sess.run(W))\n",
    "\n",
    "    print('-------------------------------')\n",
    "\n",
    "    # 1은 bias로 항상 1. (11, 7)은 x 입력\n",
    "    a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7]]})\n",
    "    print(\"a :\", a, sess.run(tf.argmax(a, 1)))         # a : [[ 0.68849683  0.26731509  0.04418806]] [0]\n",
    "\n",
    "    b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4]]})\n",
    "    print(\"b :\", b, sess.run(tf.argmax(b, 1)))         # b : [[ 0.2432227   0.44183081  0.3149465 ]] [1]\n",
    "\n",
    "    c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0]]})\n",
    "    print(\"c :\", c, sess.run(tf.argmax(c, 1)))         # c : [[ 0.02974809  0.08208466  0.8881672 ]] [2]\n",
    "\n",
    "    # 한번에 여러 개 판단 가능\n",
    "    d = sess.run(hypothesis, feed_dict={X: [[1, 11, 7], [1, 3, 4], [1, 1, 0]]})\n",
    "    print(\"d : \", *d, end=' ')\n",
    "    print(sess.run(tf.argmax(d, 1)))                   # d :  ...  [0 1 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Human's Brain\n",
    "사람의 뇌와 비슷하게 동작하도록 구성. 일정 크기 이하라면 활성화(activation)되지 않도록 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![title](deep2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![title](deep3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### XOR problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![title](xor.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Does Not Work.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.719292 [[ 0.66528422 -0.638767  ]]\n",
      "200 0.69538 [[ 0.18945464 -0.1888227 ]]\n",
      "400 0.69333 [[ 0.05405878 -0.05404442]]\n",
      "600 0.693162 [[ 0.01542732 -0.015427  ]]\n",
      "800 0.693148 [[ 0.00440274 -0.00440265]]\n",
      "--------------------------------------------------\n",
      "[[ 0.5         0.49968395  0.50031608  0.50000006]]\n",
      "[[ 1.  0.  1.  1.]]\n",
      "[[False False  True False]]\n",
      "0.25\n",
      "Accuracy : 0.25\n"
     ]
    }
   ],
   "source": [
    "# 07train.txt\n",
    "# # x1 x2 y\n",
    "# 0   0   0\n",
    "# 0   1   1\n",
    "# 1   0   1\n",
    "# 1   1   0\n",
    "\n",
    "xy = np.loadtxt('07train.txt', unpack=True)\n",
    "\n",
    "x_data = xy[:-1]\n",
    "y_data = xy[-1]\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1, len(x_data)], -1.0, 1.0))\n",
    "\n",
    "h = tf.matmul(W, X)\n",
    "hypothesis = tf.div(1., 1. + tf.exp(-h))\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "\n",
    "rate = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1000):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    "    print('-'*50)\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.floor(hypothesis+0.5), Y)\n",
    "\n",
    "    #Calculate accuraty\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    param = [hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy]\n",
    "    result = sess.run(param, feed_dict={X:x_data, Y:y_data})\n",
    "\n",
    "    for i in result:\n",
    "        print(i)\n",
    "    print('Accuracy :', accuracy.eval({X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### NN for XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000 0.69200277 [[-0.13211167  0.46434367  0.1040895  -0.87748784]] [[-0.4290899  0.7208426]]\n",
      " 3000 0.61829782 [[ 0.18880604  2.1777606  -0.30438277 -2.6517334 ]] [[-0.58828872  2.26119304]]\n",
      " 5000 0.13159674 [[ 3.95941901  4.9969511  -3.6458838  -5.17609692]] [[-5.19976091  6.1986351 ]]\n",
      " 7000 0.03919408 [[ 5.26022816  5.74764252 -4.95854044 -5.87845278]] [[-7.61965942  8.20520401]]\n",
      " 9000 0.02208670 [[ 5.7486949   6.06264639 -5.45247316 -6.20020151]] [[-8.71226215  9.22755051]]\n",
      "--------------------------------------------------\n",
      "[ 0.01744685] [ 0.9759739] [ 0.98469615] [ 0.01473866]\n",
      "[ 0.] [ 1.] [ 1.] [ 0.]\n",
      "[ True] [ True] [ True] [ True]\n",
      "1.0\n",
      "Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('07train.txt', unpack=True)\n",
    "\n",
    "x_data = np.transpose(xy[:-1])\n",
    "y_data = np.reshape(xy[-1], (4, 1))\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform([2, 2], -1.0, 1.0))\n",
    "W2 = tf.Variable(tf.random_uniform([2, 1], -1.0, 1.0))\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([2]))\n",
    "b2 = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "L2 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "hypothesis = tf.sigmoid(tf.matmul(L2, W2) + b2)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "\n",
    "rate = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(10000):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 2000 == 999:\n",
    "            # b1과 b2는 출력 생략. 한 줄에 출력하기 위해 reshape 사용\n",
    "            r1, (r2, r3) = sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run([W1, W2])\n",
    "            print('{:5} {:10.8f} {} {}'.format(step+1, r1, np.reshape(r2, (1,4)), np.reshape(r3, (1,2))))\n",
    "    print('-'*50)\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.floor(hypothesis+0.5), Y)\n",
    "\n",
    "    #Calculate accuraty\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    param = [hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy]\n",
    "    result = sess.run(param, feed_dict={X:x_data, Y:y_data})\n",
    "\n",
    "    print(*result[0])\n",
    "    print(*result[1])\n",
    "    print(*result[2])\n",
    "    print( result[-1])\n",
    "    print('Accuracy :', accuracy.eval({X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost= 0.548379772\n",
      "Epoch: 0002 cost= 0.365997937\n",
      "Epoch: 0003 cost= 0.336678850\n",
      "Epoch: 0004 cost= 0.321124137\n",
      "Epoch: 0005 cost= 0.311562937\n",
      "Epoch: 0006 cost= 0.304303360\n",
      "Epoch: 0007 cost= 0.298768657\n",
      "Epoch: 0008 cost= 0.294053870\n",
      "Epoch: 0009 cost= 0.290711079\n",
      "Epoch: 0010 cost= 0.287628350\n",
      "Epoch: 0011 cost= 0.285078243\n",
      "Epoch: 0012 cost= 0.282728591\n",
      "Epoch: 0013 cost= 0.280666281\n",
      "Epoch: 0014 cost= 0.278552884\n",
      "Epoch: 0015 cost= 0.277076334\n",
      "Epoch: 0016 cost= 0.275575479\n",
      "Epoch: 0017 cost= 0.274273545\n",
      "Epoch: 0018 cost= 0.272780013\n",
      "Epoch: 0019 cost= 0.271811423\n",
      "Epoch: 0020 cost= 0.270582262\n",
      "Epoch: 0021 cost= 0.269584267\n",
      "Epoch: 0022 cost= 0.268606179\n",
      "Epoch: 0023 cost= 0.267941275\n",
      "Epoch: 0024 cost= 0.266838480\n",
      "Epoch: 0025 cost= 0.266117674\n",
      "Label :  [9]\n",
      "Prediction : [9]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1176fcc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "Accuracy: 0.924\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# Parameters. 반복문에서 사용하는데, 미리 만들어 놓았다.\n",
    "learning_rate = 0.1\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10])  # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Construct model\n",
    "activation = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(activation), reduction_indices=1))\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        # 나누어 떨어지지 않으면, 뒤쪽 이미지 일부는 사용하지 않는다.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs, y: batch_ys})\n",
    "\n",
    "            # 분할해서 구동하기 때문에 cost를 계속해서 누적시킨다. 전체 중의 일부에 대한 비용.\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step. display_step이 1이기 때문에 if는 필요없다.\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    # 추가한 코드. Label과 Prediction이 같은 값을 출력하면 맞는 것이다.\n",
    "    import random\n",
    "    r = random.randrange(mnist.test.num_examples)\n",
    "    print('Label : ', sess.run(tf.argmax(mnist.test.labels[r:r+1], 1)))\n",
    "    print('Prediction :', sess.run(tf.argmax(activation, 1), {x: mnist.test.images[r:r+1]}))\n",
    "\n",
    "    # 1줄로 된 것을 28x28로 변환\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(activation, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost= 171.527372927\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# Parameters. 반복문에서 사용하는데, 미리 만들어 놓았다.\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph Input\n",
    "X = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "Y = tf.placeholder(tf.float32, [None, 10])  # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# --------------------------- 수정한 부분 ------------------------------ #\n",
    "# Set model weights\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "W3 = tf.Variable(tf.random_normal([256,  10]))\n",
    "\n",
    "B1 = tf.Variable(tf.random_normal([256]))\n",
    "B2 = tf.Variable(tf.random_normal([256]))\n",
    "B3 = tf.Variable(tf.random_normal([ 10]))\n",
    "\n",
    "# Construct model\n",
    "L1 = tf.nn.relu(tf.add(tf.matmul(X, W1), B1))\n",
    "L2 = tf.nn.relu(tf.add(tf.matmul(L1, W2), B2)) # Hidden layer with ReLU activation\n",
    "hypothesis = tf.add(tf.matmul(L2, W3), B3)     # No need to use softmax here\n",
    "# ---------------------------- 여기까지 ------------------------------- #\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))   # softmax loss\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        # 나누어 떨어지지 않으면, 뒤쪽 이미지 일부는 사용하지 않는다.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "\n",
    "            # 분할해서 구동하기 때문에 cost를 계속해서 누적시킨다. 전체 중의 일부에 대한 비용.\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step. display_step이 1이기 때문에 if는 필요없다.\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    # 추가한 코드. Label과 Prediction이 같은 값을 출력하면 맞는 것이다.\n",
    "    import random\n",
    "    r = random.randrange(mnist.test.num_examples)\n",
    "    print('Label : ', sess.run(tf.argmax(mnist.test.labels[r:r+1], 1)))\n",
    "    print('Prediction :', sess.run(tf.argmax(hypothesis, 1), {X: mnist.test.images[r:r+1]}))\n",
    "\n",
    "    # 1줄로 된 것을 28x28로 변환\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Next.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
