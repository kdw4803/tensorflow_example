{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Machine Learning (feat. Tensorflow)\n",
    "\n",
    "2017.4.13\n",
    "Dongwoo 화목회"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is the subfield of computer science that gives computers the ability to learn without being explicitly programmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised vs Unsupervised ( vs reinforcement )\n",
    "- supervised\n",
    "    - Regression problem\n",
    "    - Classification problem\n",
    "- unsupervised\n",
    "    - Clustering\n",
    "- reinforecement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow\n",
    "Tensorflow is an open-source software library for Machine Intelligence. It was developed by Google to meet their needs for systems capable of building and training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Start.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "H(x) = Wx + b에서 Wx + b는 x에 대한 1차 방적식으로 직선을 표현한다. 기울기에 해당하는 W(Weight)와 절편에 해당하는 b(bias)가 반복되는 과정에서 계속 바뀌고, 마지막 루프에서 바뀐 최종 값을 사용해서 데이터 예측(prediction)에 사용하게 된다. 최종 결과로 나온 가설을 모델(model)이라고 부르고, \"학습되었다\"라고 한다.\n",
    "![title](hypothesis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "앞에서 설명한 Hypothesis 방정식에 대한 비용(cost)으로 방정식의 결과가 크게 나오면 좋지 않다고 얘기하고, 루프를 돌 때마다 W와 b를 비용이 적게 발생하는 방향으로 수정하게 된다.\n",
    "![title](cost_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal : Minimize cost\n",
    "목표는 cost를 최소로 만드는 W(기울기)와 b(절편)를 찾는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "2차원 좌표에 분포된 데이터를 1차원 직선 방정식을 통해 표현되지 않은 데이터를 예측하기 위한 분석 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](linear1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](linear2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 163.948 [ 12.65646839] [-31.33131599]\n",
      "400 49.4119 [ 7.85027409] [-17.20051956]\n",
      "600 14.8921 [ 5.21173] [-9.44287872]\n",
      "800 4.48831 [ 3.76320076] [-5.18402481]\n",
      "1000 1.35272 [ 2.96797681] [-2.84596992]\n",
      "1200 0.407695 [ 2.53140807] [-1.5624038]\n",
      "1400 0.122874 [ 2.2917366] [-0.857741]\n",
      "1600 0.0370329 [ 2.16016006] [-0.47088954]\n",
      "1800 0.0111613 [ 2.08792591] [-0.25851271]\n",
      "2000 0.00336386 [ 2.04827023] [-0.14192067]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwlEDPzniCyIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d365cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_data = [1., 2., 3., 4.]\n",
    "y_data = [2., 4., 6., 8.]\n",
    "\n",
    "# range is -100 ~ 100\n",
    "W = tf.Variable(tf.random_uniform([1], -100., 100.))\n",
    "b = tf.Variable(tf.random_uniform([1], -100., 100.))\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "hypothesis = W * X + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "rate = tf.Variable(0.01)\n",
    "optimizer = tf.train.GradientDescentOptimizer(rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(2001):\n",
    "    sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 200 == 0 and step >= 200:\n",
    "        print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W), sess.run(b))\n",
    "        plt.plot(x_data,sess.run(W)*x_data + sess.run(b))\n",
    "\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x')\n",
    "plt.show()\n",
    "\n",
    "# print(sess.run(hypothesis, feed_dict={X: 5}))           # [ 10.]\n",
    "# print(sess.run(hypothesis, feed_dict={X: 2.5}))         # [5.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Classification\n",
    "Linear Regression을 활용해서 데이터를 분류하는 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid\n",
    "sigmoid는 linear regression에서 가져온 값을 0과 1 사이의 값으로 변환한다. x가 0일 때, 0.5가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New cost function\n",
    "sigmoid 함수 적용으로 인해 새로운 cost function를 사용해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](cost_function2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Decent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](gradient.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.898549 [[-0.55525601  0.0623652   0.68821198]]\n",
      "200 0.45219 [[-2.1853776   0.19849353  0.44756472]]\n",
      "400 0.403499 [[-3.15210986  0.28918573  0.58467913]]\n",
      "600 0.379671 [[-3.83011079  0.34572631  0.68628526]]\n",
      "800 0.365941 [[-4.34534836  0.38418618  0.76665646]]\n",
      "1000 0.357153 [[-4.75775862  0.41196236  0.83298123]]\n",
      "1200 0.351106 [[-5.0999198   0.43291426  0.88934517]]\n",
      "1400 0.346722 [[-5.39130163  0.44924662  0.93828434]]\n",
      "1600 0.343413 [[-5.6444211   0.46231186  0.98148257]]\n",
      "1800 0.340836 [[-5.86775255  0.47298369  1.02011228]]\n",
      "2000 0.338779 [[-6.06729364  0.48185331  1.05502284]]\n",
      "-----------------------------------------\n",
      "[1, 2, 1] : [[False]]\n",
      "[1, 5, 5] : [[ True]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 04train.txt\n",
    "# #x0 x1 x2 y\n",
    "# 1   2   1   0\n",
    "# 1   3   2   0\n",
    "# 1   3   5   0\n",
    "# 1   5   5   1\n",
    "# 1   7   5   1\n",
    "# 1   2   5   1\n",
    "\n",
    "# 원본 파일은 6행 4열이지만, 열 우선이라서 4행 6열로 가져옴\n",
    "xy = np.loadtxt('04train.txt', unpack=True, dtype='float32')\n",
    "\n",
    "# print(xy[0], xy[-1])        # [ 1.  1.  1.  1.  1.  1.] [ 0.  0.  0.  1.  1.  1.]\n",
    "\n",
    "x_data = xy[:-1]            # 3행 6열\n",
    "y_data = xy[-1]             # 1행 6열\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# feature별 가중치를 난수로 초기화. feature는 bias 포함해서 3개. 1행 3열.\n",
    "W = tf.Variable(tf.random_uniform([1, len(x_data)], -1.0, 1.0))\n",
    "\n",
    "# 행렬 곱셈. (1x3) * (3x6)\n",
    "h = tf.matmul(W, X)\n",
    "hypothesis = tf.div(1., 1. + tf.exp(-h))    # exp(-h) = e ** -h. e는 자연상수\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "\n",
    "rate = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(2001):\n",
    "    sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 200 == 0:\n",
    "        print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    "\n",
    "print('-----------------------------------------')\n",
    "\n",
    "# 결과가 0 또는 1로 계산되는 것이 아니라 0과 1 사이의 값으로 나오기 때문에 True/False는 직접 판단\n",
    "print('[1, 2, 1] :', sess.run(hypothesis, feed_dict={X: [[1], [2], [1]]}) > 0.5)\n",
    "print('[1, 5, 5] :', sess.run(hypothesis, feed_dict={X: [[1], [5], [5]]}) > 0.5)\n",
    "sess.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression\n",
    "Logistic Regression을 부르는 다른 이름은 binary classification이다. 데이터를 1과 0의 두 가지 그룹으로 나누기 위해 사용하는 모델이다. Softmax는 데이터를 2개 이상의 그룹으로 나누기 위해 binary classification을 확장한 모델이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](softmax1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "softmax는 점수로 나온 결과를 전체 합계가 1이 되는 0과 1 사이의 값으로 변경해 준다. 전체를 더하면 1이 되기 때문에 확률(probabilites)이라고 부르면 의미가 더욱 분명해진다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](softmax2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "one-hot encoding은 softmax로 구한 값 중에서 가장 큰 값을 1로, 나머지를 0으로 만든다. 어떤 것을 선택할지를 확실하게 정리해 준다. one-hot encoding은 설명한 것처럼 매우 간단하기 때문에 직접 구현할 수도 있지만, 텐서플로우에서는 argmax 함수라는 이름으로 제공하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](softmax3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy Cost Function\n",
    "S(Y)는 softmax가 예측한 값이고, L(Y)는 실제 Y의 값으로 L은 label을 의미한다. cost 함수는 예측한 값과 실제 값의 거리(distance, D)를 계산하는 함수로, 이 값이 줄어드는 방향으로, 즉 entropy가 감소하는 방향으로 진행하다 보면 최저점을 만나게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](softmax4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](softmax5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1.0678 [-0.00833333  0.00416667  0.00416666] [ 0.01666667  0.02916667 -0.04583334] [ 0.01666666  0.04166667 -0.05833334]\n",
      " 200 0.699681 [-1.5737735  -0.36410642  1.93788099] [ 0.0967759  -0.09235884 -0.00441682] [ 0.24915566  0.23823613 -0.48739177]\n",
      " 400 0.594579 [-2.54309034 -0.43173078  2.97482228] [ 0.13064831 -0.04841127 -0.08223619] [ 0.40734777  0.22857714 -0.63592446]\n",
      " 600 0.535829 [-3.31521416 -0.39115471  3.70637012] [ 0.13982886 -0.02464214 -0.11518568] [ 0.54843497  0.21297167 -0.76140594]\n",
      " 800 0.494312 [-3.98393679 -0.31143472  4.29537106] [ 0.1417881  -0.00935038 -0.13243635] [ 0.6748786   0.1950776  -0.86995488]\n",
      "1000 0.461916 [-4.58334923 -0.21852757  4.8018775 ] [ 0.14145668  0.00154199 -0.14299677] [ 0.78935486  0.17700934 -0.9663626 ]\n",
      "1200 0.435343 [-5.13021469 -0.12343737  5.25365353] [ 0.14044461  0.00979479 -0.15023731] [ 0.89399701  0.15975396 -1.05374885]\n",
      "1400 0.412891 [-5.63466215 -0.03106253  5.66572714] [ 0.1393245   0.01630212 -0.15562418] [ 0.99044132  0.14372151 -1.13416028]\n",
      "1600 0.393535 [-6.10366583  0.05645988  6.04720736] [ 0.13829574  0.02157287 -0.15986556] [ 1.07996321  0.12904176 -1.20900333]\n",
      "1800 0.376595 [-6.54242659  0.13831529  6.40411282] [ 0.13741526  0.02592302 -0.16333508] [ 1.16357517  0.11571171 -1.27928567]\n",
      "2000 0.361591 [-6.95501471  0.21433842  6.7406764 ] [ 0.13668649  0.02956286 -0.16624542] [ 1.2420913  0.1036661 -1.3457557]\n",
      "2200 0.348169 [-7.34470749  0.28468168  7.06002617] [ 0.13609275  0.03263915 -0.16872707] [ 1.31617439  0.09281436 -1.40898621]\n",
      "2400  0.33606 [-7.71420002  0.34964585  7.36455774] [ 0.13561255  0.03525931 -0.17086639] [ 1.38636887  0.08305811 -1.46942413]\n",
      "2600 0.325053 [-8.06574821  0.40959236  7.65615749] [ 0.13522536  0.03750427 -0.17272341] [ 1.45312846  0.07430083 -1.52742565]\n",
      "2800 0.314984 [-8.40124512  0.46489772  7.93635178] [ 0.13491319  0.03943677 -0.17434347] [ 1.51683164  0.06645141 -1.58327949]\n",
      "3000 0.305719 [-8.72231579  0.51593047  8.20638561] [ 0.13466108  0.04110633 -0.17576036] [ 1.57780051  0.0594258  -1.63722253]\n",
      "-------------------------------\n",
      "a : [[  9.18195665e-01   8.16900209e-02   1.14327857e-04]] [0]\n",
      "b : [[ 0.21065326  0.68458456  0.10476214]] [1]\n",
      "c : [[  6.06074408e-08   5.67564450e-04   9.99432385e-01]] [2]\n"
     ]
    }
   ],
   "source": [
    "# softmax이기 때문에 y를 표현할 때, 벡터로 표현한다.\n",
    "# 1개의 값으로 표현한다고 할 때, 뭐라고 쓸지도 사실 애매하다.\n",
    "\n",
    "# 05train.txt\n",
    "# #x0 x1 x2 y[A   B   C]\n",
    "# 1   2   1   0   0   1     # C\n",
    "# 1   3   2   0   0   1\n",
    "# 1   3   4   0   0   1\n",
    "# 1   5   5   0   1   0     # B\n",
    "# 1   7   5   0   1   0\n",
    "# 1   2   5   0   1   0\n",
    "# 1   6   6   1   0   0     # A\n",
    "# 1   7   7   1   0   0\n",
    "\n",
    "xy = np.loadtxt('05train.txt', unpack=True, dtype='float32')\n",
    "\n",
    "# xy는 6x8. xy[:3]은 3x8. 행렬 곱셈을 하기 위해 미리 transpose.\n",
    "x_data = np.transpose(xy[:3])\n",
    "y_data = np.transpose(xy[3:])\n",
    "\n",
    "# print('x_data :', x_data.shape)     # x_data : (8, 3)\n",
    "# print('y_data :', y_data.shape)     # y_data : (8, 3)\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])  # x_data와 같은 크기의 열 가짐. 행 크기는 모름.\n",
    "Y = tf.placeholder(\"float\", [None, 3])  # tf.float32라고 써도 됨\n",
    "\n",
    "W = tf.Variable(tf.zeros([3, 3]))       # 3x3 행렬. 전체 0.\n",
    "\n",
    "# softmax 알고리즘 적용. X*W = (8x3) * (3x3) = (8x3)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W))\n",
    "\n",
    "# cross-entropy cost 함수\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), reduction_indices=1))\n",
    "\n",
    "learning_rate = 0.1\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(3001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            feed = {X: x_data, Y: y_data}\n",
    "            print('{:4} {:8.6}'.format(step, sess.run(cost, feed_dict=feed)), *sess.run(W))\n",
    "\n",
    "    print('-------------------------------')\n",
    "\n",
    "    # 1은 bias로 항상 1. (11, 7)은 x 입력\n",
    "    a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7]]})\n",
    "    print(\"a :\", a, sess.run(tf.argmax(a, 1)))         \n",
    "\n",
    "    b = sess.run(hypothesis, feed_dict={X: [[1, 5, 5]]})\n",
    "    print(\"b :\", b, sess.run(tf.argmax(b, 1)))         \n",
    "\n",
    "    c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0]]})\n",
    "    print(\"c :\", c, sess.run(tf.argmax(c, 1)))        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Human's Brain\n",
    "사람의 뇌와 비슷하게 동작하도록 구성. 일정 크기 이하라면 활성화(activation)되지 않도록 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](deep2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](deep3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](xor.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does Not Work.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.719292 [[ 0.66528422 -0.638767  ]]\n",
      "200 0.69538 [[ 0.18945464 -0.1888227 ]]\n",
      "400 0.69333 [[ 0.05405878 -0.05404442]]\n",
      "600 0.693162 [[ 0.01542732 -0.015427  ]]\n",
      "800 0.693148 [[ 0.00440274 -0.00440265]]\n",
      "--------------------------------------------------\n",
      "[[ 0.5         0.49968395  0.50031608  0.50000006]]\n",
      "[[ 1.  0.  1.  1.]]\n",
      "[[False False  True False]]\n",
      "0.25\n",
      "Accuracy : 0.25\n"
     ]
    }
   ],
   "source": [
    "# 07train.txt\n",
    "# # x1 x2 y\n",
    "# 0   0   0\n",
    "# 0   1   1\n",
    "# 1   0   1\n",
    "# 1   1   0\n",
    "\n",
    "xy = np.loadtxt('07train.txt', unpack=True)\n",
    "\n",
    "x_data = xy[:-1]\n",
    "y_data = xy[-1]\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1, len(x_data)], -1.0, 1.0))\n",
    "\n",
    "h = tf.matmul(W, X)\n",
    "hypothesis = tf.div(1., 1. + tf.exp(-h))\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "\n",
    "rate = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1000):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    "    print('-'*50)\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.floor(hypothesis+0.5), Y)\n",
    "\n",
    "    #Calculate accuraty\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    param = [hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy]\n",
    "    result = sess.run(param, feed_dict={X:x_data, Y:y_data})\n",
    "\n",
    "    for i in result:\n",
    "        print(i)\n",
    "    print('Accuracy :', accuracy.eval({X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN for XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000 0.69361567 [[-0.62957144 -0.63341898 -0.28585973 -0.44579151]] [[ 0.25430468  0.06575769]]\n",
      " 3000 0.69157255 [[-0.55039185 -0.78672272 -0.15172052 -0.63336241]] [[ 0.20783927 -0.37313291]]\n",
      " 5000 0.56416512 [[-0.81365776 -3.07515717 -0.37181348 -3.0292902 ]] [[ 0.89588499 -3.19002724]]\n",
      " 7000 0.14907683 [[-2.92574763 -5.33899879 -2.91690612 -5.2876029 ]] [[ 5.46567297 -6.81815004]]\n",
      " 9000 0.04340918 [[-4.09327698 -5.98034191 -4.0894537  -5.9538064 ]] [[ 7.96992588 -8.7071104 ]]\n",
      "--------------------------------------------------\n",
      "[ 0.02253103] [ 0.97256672] [ 0.97261804] [ 0.0442754]\n",
      "[ 0.] [ 1.] [ 1.] [ 0.]\n",
      "[ True] [ True] [ True] [ True]\n",
      "1.0\n",
      "Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('07train.txt', unpack=True)\n",
    "\n",
    "x_data = np.transpose(xy[:-1])\n",
    "y_data = np.reshape(xy[-1], (4, 1))\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform([2, 2], -1.0, 1.0))\n",
    "W2 = tf.Variable(tf.random_uniform([2, 1], -1.0, 1.0))\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([2]))\n",
    "b2 = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "L2 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "hypothesis = tf.sigmoid(tf.matmul(L2, W2) + b2)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "\n",
    "rate = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(10000):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 2000 == 999:\n",
    "            # b1과 b2는 출력 생략. 한 줄에 출력하기 위해 reshape 사용\n",
    "            r1, (r2, r3) = sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run([W1, W2])\n",
    "            print('{:5} {:10.8f} {} {}'.format(step+1, r1, np.reshape(r2, (1,4)), np.reshape(r3, (1,2))))\n",
    "    print('-'*50)\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.floor(hypothesis+0.5), Y)\n",
    "\n",
    "    #Calculate accuraty\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    param = [hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy]\n",
    "    result = sess.run(param, feed_dict={X:x_data, Y:y_data})\n",
    "\n",
    "    print(*result[0])\n",
    "    print(*result[1])\n",
    "    print(*result[2])\n",
    "    print( result[-1])\n",
    "    print('Accuracy :', accuracy.eval({X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost= 0.548379772\n",
      "Epoch: 0002 cost= 0.365997937\n",
      "Epoch: 0003 cost= 0.336678850\n",
      "Epoch: 0004 cost= 0.321124137\n",
      "Epoch: 0005 cost= 0.311562937\n",
      "Epoch: 0006 cost= 0.304303360\n",
      "Epoch: 0007 cost= 0.298768657\n",
      "Epoch: 0008 cost= 0.294053870\n",
      "Epoch: 0009 cost= 0.290711079\n",
      "Epoch: 0010 cost= 0.287628350\n",
      "Epoch: 0011 cost= 0.285078243\n",
      "Epoch: 0012 cost= 0.282728591\n",
      "Epoch: 0013 cost= 0.280666281\n",
      "Epoch: 0014 cost= 0.278552884\n",
      "Epoch: 0015 cost= 0.277076334\n",
      "Epoch: 0016 cost= 0.275575479\n",
      "Epoch: 0017 cost= 0.274273545\n",
      "Epoch: 0018 cost= 0.272780013\n",
      "Epoch: 0019 cost= 0.271811423\n",
      "Epoch: 0020 cost= 0.270582262\n",
      "Epoch: 0021 cost= 0.269584267\n",
      "Epoch: 0022 cost= 0.268606179\n",
      "Epoch: 0023 cost= 0.267941275\n",
      "Epoch: 0024 cost= 0.266838480\n",
      "Epoch: 0025 cost= 0.266117674\n",
      "Label :  [9]\n",
      "Prediction : [9]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1176fcc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "Accuracy: 0.924\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# Parameters.\n",
    "learning_rate = 0.1\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10])  # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Construct model\n",
    "hypothesis = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(hypothesis), reduction_indices=1))\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        # 나누어 떨어지지 않으면, 뒤쪽 이미지 일부는 사용하지 않는다.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs, y: batch_ys})\n",
    "\n",
    "            # 분할해서 구동하기 때문에 cost를 계속해서 누적시킨다. 전체 중의 일부에 대한 비용.\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step. display_step이 1이기 때문에 if는 필요없다.\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    # Label과 Prediction이 같은 값을 출력하면 맞는 것이다.\n",
    "    import random\n",
    "    r = random.randrange(mnist.test.num_examples)\n",
    "    print('Label : ', sess.run(tf.argmax(mnist.test.labels[r:r+1], 1)))\n",
    "    print('Prediction :', sess.run(tf.argmax(hypothesis, 1), {x: mnist.test.images[r:r+1]}))\n",
    "\n",
    "    # 1줄로 된 것을 28x28로 변환\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# Parameters. 반복문에서 사용하는데, 미리 만들어 놓았다.\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph Input\n",
    "X = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "Y = tf.placeholder(tf.float32, [None, 10])  # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# --------------------------- 수정한 부분 ------------------------------ #\n",
    "# Set model weights\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "W3 = tf.Variable(tf.random_normal([256,  10]))\n",
    "\n",
    "B1 = tf.Variable(tf.random_normal([256]))\n",
    "B2 = tf.Variable(tf.random_normal([256]))\n",
    "B3 = tf.Variable(tf.random_normal([ 10]))\n",
    "\n",
    "# Construct model\n",
    "L1 = tf.nn.relu(tf.add(tf.matmul(X, W1), B1))\n",
    "L2 = tf.nn.relu(tf.add(tf.matmul(L1, W2), B2)) # Hidden layer with ReLU activation\n",
    "hypothesis = tf.add(tf.matmul(L2, W3), B3)     # No need to use softmax here\n",
    "# ---------------------------- 여기까지 ------------------------------- #\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))   # softmax loss\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        # 나누어 떨어지지 않으면, 뒤쪽 이미지 일부는 사용하지 않는다.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "\n",
    "            # 분할해서 구동하기 때문에 cost를 계속해서 누적시킨다. 전체 중의 일부에 대한 비용.\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step. display_step이 1이기 때문에 if는 필요없다.\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    # Label과 Prediction이 같은 값을 출력하면 맞는 것이다.\n",
    "    import random\n",
    "    r = random.randrange(mnist.test.num_examples)\n",
    "    print('Label : ', sess.run(tf.argmax(mnist.test.labels[r:r+1], 1)))\n",
    "    print('Prediction :', sess.run(tf.argmax(hypothesis, 1), {X: mnist.test.images[r:r+1]}))\n",
    "\n",
    "    # 1줄로 된 것을 28x28로 변환\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Next.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
