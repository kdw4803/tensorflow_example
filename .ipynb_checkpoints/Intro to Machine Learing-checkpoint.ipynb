{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Machine Learning (feat. Tensorflow)\n",
    "\n",
    "2017.4.13\n",
    "Dongwoo 화목회"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is the subfield of computer science that gives computers the ability to learn without being explicitly programmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised vs Unsupervised ( vs reinforcement )\n",
    "- supervised\n",
    "    - Regression problem\n",
    "    - Classification problem\n",
    "- unsupervised\n",
    "    - Clustering\n",
    "- reinforecement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow\n",
    "Tensorflow is an open-source software library for Machine Intelligence. It was developed by Google to meet their needs for systems capable of building and training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Start.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "H(x) = Wx + b에서 Wx + b는 x에 대한 1차 방적식으로 직선을 표현한다. 기울기에 해당하는 W(Weight)와 절편에 해당하는 b(bias)가 반복되는 과정에서 계속 바뀌고, 마지막 루프에서 바뀐 최종 값을 사용해서 데이터 예측(prediction)에 사용하게 된다. 최종 결과로 나온 가설을 모델(model)이라고 부르고, \"학습되었다\"라고 한다.\n",
    "![title](hypothesis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "앞에서 설명한 Hypothesis 방정식에 대한 비용(cost)으로 방정식의 결과가 크게 나오면 좋지 않다고 얘기하고, 루프를 돌 때마다 W와 b를 비용이 적게 발생하는 방향으로 수정하게 된다.\n",
    "![title](cost_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal : Minimize cost\n",
    "목표는 cost를 최소로 만드는 W(기울기)와 b(절편)를 찾는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "2차원 좌표에 분포된 데이터를 1차원 직선 방정식을 통해 표현되지 않은 데이터를 예측하기 위한 분석 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](linear1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](linear2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 206.674 [-9.96474075] [ 35.17779541]\n",
      "400 62.289 [-4.56849813] [ 19.31218338]\n",
      "600 18.7731 [-1.60602582] [ 10.60215569]\n",
      "800 5.65799 [ 0.02033605] [ 5.82045364]\n",
      "1000 1.70525 [ 0.91318834] [ 3.19535875]\n",
      "1200 0.513941 [ 1.40335345] [ 1.75421357]\n",
      "1400 0.154896 [ 1.67244816] [ 0.9630428]\n",
      "1600 0.0466837 [ 1.82017791] [ 0.52869904]\n",
      "1800 0.0140699 [ 1.90127993] [ 0.2902492]\n",
      "2000 0.0042405 [ 1.94580364] [ 0.15934351]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEKCAYAAAASByJ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCAHA/wfhvXbF2FGaLAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122b57b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_data = [1., 2., 3., 4.]\n",
    "y_data = [2., 4., 6., 8.]\n",
    "\n",
    "# range is -100 ~ 100\n",
    "W = tf.Variable(tf.random_uniform([1], -100., 100.))\n",
    "b = tf.Variable(tf.random_uniform([1], -100., 100.))\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "hypothesis = W * X + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "rate = tf.Variable(0.01)\n",
    "optimizer = tf.train.GradientDescentOptimizer(rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(2001):\n",
    "    sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 200 == 0 and step >= 200:\n",
    "        print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W), sess.run(b))\n",
    "        plt.plot(x_data,sess.run(W)*x_data + sess.run(b))\n",
    "\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x')\n",
    "plt.show()\n",
    "\n",
    "# print(sess.run(hypothesis, feed_dict={X: 5}))           # [ 10.]\n",
    "# print(sess.run(hypothesis, feed_dict={X: 2.5}))         # [5.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Classification\n",
    "Linear Regression을 활용해서 데이터를 분류하는 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid\n",
    "sigmoid는 linear regression에서 가져온 값을 0과 1 사이의 값으로 변환한다. x가 0일 때, 0.5가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New cost function\n",
    "sigmoid 함수 적용으로 인해 새로운 cost function를 사용해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](cost_function2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Decent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](gradient.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.837924 [[ 0.67223454 -0.10460093  0.3180154 ]]\n",
      "200 0.500997 [[-1.48802495  0.12843084  0.35273504]]\n",
      "400 0.423672 [[-2.7033565   0.24845743  0.51990491]]\n",
      "600 0.390119 [[-3.50705624  0.31958625  0.63729244]]\n",
      "800 0.372174 [[-4.09583426  0.3660709   0.72739005]]\n",
      "1000 0.361234 [[-4.55587435  0.39870661  0.80029243]]\n",
      "1200 0.35396 [[-4.93113947  0.4228183   0.86139172]]\n",
      "1400 0.348815 [[-5.24675035  0.44131786  0.91389936]]\n",
      "1600 0.345007 [[-5.51830196  0.45593175  0.95987999]]\n",
      "1800 0.342086 [[-5.75608969  0.46774739  1.0007385 ]]\n",
      "2000 0.339783 [[-5.96724463  0.47748369  1.03747296]]\n",
      "-----------------------------------------\n",
      "[1, 2, 1] : [[False]]\n",
      "[1, 5, 5] : [[ True]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 04train.txt\n",
    "# #x0 x1 x2 y\n",
    "# 1   2   1   0\n",
    "# 1   3   2   0\n",
    "# 1   3   5   0\n",
    "# 1   5   5   1\n",
    "# 1   7   5   1\n",
    "# 1   2   5   1\n",
    "\n",
    "# 원본 파일은 6행 4열이지만, 열 우선이라서 4행 6열로 가져옴\n",
    "xy = np.loadtxt('04train.txt', unpack=True, dtype='float32')\n",
    "\n",
    "# print(xy[0], xy[-1])        # [ 1.  1.  1.  1.  1.  1.] [ 0.  0.  0.  1.  1.  1.]\n",
    "\n",
    "x_data = xy[:-1]            # 3행 6열\n",
    "y_data = xy[-1]             # 1행 6열\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# feature별 가중치를 난수로 초기화. feature는 bias 포함해서 3개. 1행 3열.\n",
    "W = tf.Variable(tf.random_uniform([1, len(x_data)], -1.0, 1.0))\n",
    "\n",
    "# 행렬 곱셈. (1x3) * (3x6)\n",
    "h = tf.matmul(W, X)\n",
    "hypothesis = tf.div(1., 1. + tf.exp(-h))    # exp(-h) = e ** -h. e는 자연상수\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "\n",
    "rate = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(2001):\n",
    "    sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 200 == 0:\n",
    "        print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    "\n",
    "print('-----------------------------------------')\n",
    "\n",
    "# 결과가 0 또는 1로 계산되는 것이 아니라 0과 1 사이의 값으로 나오기 때문에 True/False는 직접 판단\n",
    "print('[1, 2, 1] :', sess.run(hypothesis, feed_dict={X: [[1], [2], [1]]}) > 0.5)\n",
    "print('[1, 5, 5] :', sess.run(hypothesis, feed_dict={X: [[1], [5], [5]]}) > 0.5)\n",
    "sess.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression\n",
    "Logistic Regression을 부르는 다른 이름은 binary classification이다. 데이터를 1과 0의 두 가지 그룹으로 나누기 위해 사용하는 모델이다. Softmax는 데이터를 2개 이상의 그룹으로 나누기 위해 binary classification을 확장한 모델이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](softmax1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "softmax는 점수로 나온 결과를 전체 합계가 1이 되는 0과 1 사이의 값으로 변경해 준다. 전체를 더하면 1이 되기 때문에 확률(probabilites)이라고 부르면 의미가 더욱 분명해진다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](softmax2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "one-hot encoding은 softmax로 구한 값 중에서 가장 큰 값을 1로, 나머지를 0으로 만든다. 어떤 것을 선택할지를 확실하게 정리해 준다. one-hot encoding은 설명한 것처럼 매우 간단하기 때문에 직접 구현할 수도 있지만, 텐서플로우에서는 argmax 함수라는 이름으로 제공하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](softmax3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy Cost Function\n",
    "S(Y)는 softmax가 예측한 값이고, L(Y)는 실제 Y의 값으로 L은 label을 의미한다. cost 함수는 예측한 값과 실제 값의 거리(distance, D)를 계산하는 함수로, 이 값이 줄어드는 방향으로, 즉 entropy가 감소하는 방향으로 진행하다 보면 최저점을 만나게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](softmax4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](softmax5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1.0678 [-0.00833333  0.00416667  0.00416666] [ 0.01666667  0.02916667 -0.04583334] [ 0.01666666  0.04166667 -0.05833334]\n",
      " 200 0.699681 [-1.5737735  -0.36410642  1.93788099] [ 0.0967759  -0.09235884 -0.00441682] [ 0.24915566  0.23823613 -0.48739177]\n",
      " 400 0.594579 [-2.54309034 -0.43173078  2.97482228] [ 0.13064831 -0.04841127 -0.08223619] [ 0.40734777  0.22857714 -0.63592446]\n",
      " 600 0.535829 [-3.31521416 -0.39115471  3.70637012] [ 0.13982886 -0.02464214 -0.11518568] [ 0.54843497  0.21297167 -0.76140594]\n",
      " 800 0.494312 [-3.98393679 -0.31143472  4.29537106] [ 0.1417881  -0.00935038 -0.13243635] [ 0.6748786   0.1950776  -0.86995488]\n",
      "1000 0.461916 [-4.58334923 -0.21852757  4.8018775 ] [ 0.14145668  0.00154199 -0.14299677] [ 0.78935486  0.17700934 -0.9663626 ]\n",
      "1200 0.435343 [-5.13021469 -0.12343737  5.25365353] [ 0.14044461  0.00979479 -0.15023731] [ 0.89399701  0.15975396 -1.05374885]\n",
      "1400 0.412891 [-5.63466215 -0.03106253  5.66572714] [ 0.1393245   0.01630212 -0.15562418] [ 0.99044132  0.14372151 -1.13416028]\n",
      "1600 0.393535 [-6.10366583  0.05645988  6.04720736] [ 0.13829574  0.02157287 -0.15986556] [ 1.07996321  0.12904176 -1.20900333]\n",
      "1800 0.376595 [-6.54242659  0.13831529  6.40411282] [ 0.13741526  0.02592302 -0.16333508] [ 1.16357517  0.11571171 -1.27928567]\n",
      "2000 0.361591 [-6.95501471  0.21433842  6.7406764 ] [ 0.13668649  0.02956286 -0.16624542] [ 1.2420913  0.1036661 -1.3457557]\n",
      "2200 0.348169 [-7.34470749  0.28468168  7.06002617] [ 0.13609275  0.03263915 -0.16872707] [ 1.31617439  0.09281436 -1.40898621]\n",
      "2400  0.33606 [-7.71420002  0.34964585  7.36455774] [ 0.13561255  0.03525931 -0.17086639] [ 1.38636887  0.08305811 -1.46942413]\n",
      "2600 0.325053 [-8.06574821  0.40959236  7.65615749] [ 0.13522536  0.03750427 -0.17272341] [ 1.45312846  0.07430083 -1.52742565]\n",
      "2800 0.314984 [-8.40124512  0.46489772  7.93635178] [ 0.13491319  0.03943677 -0.17434347] [ 1.51683164  0.06645141 -1.58327949]\n",
      "3000 0.305719 [-8.72231579  0.51593047  8.20638561] [ 0.13466108  0.04110633 -0.17576036] [ 1.57780051  0.0594258  -1.63722253]\n",
      "-------------------------------\n",
      "a : [[  9.18195665e-01   8.16900209e-02   1.14327857e-04]] [0]\n",
      "b : [[ 0.21065326  0.68458456  0.10476214]] [1]\n",
      "c : [[  6.06074408e-08   5.67564450e-04   9.99432385e-01]] [2]\n"
     ]
    }
   ],
   "source": [
    "# softmax이기 때문에 y를 표현할 때, 벡터로 표현한다.\n",
    "# 1개의 값으로 표현한다고 할 때, 뭐라고 쓸지도 사실 애매하다.\n",
    "\n",
    "# 05train.txt\n",
    "# #x0 x1 x2 y[A   B   C]\n",
    "# 1   2   1   0   0   1     # C\n",
    "# 1   3   2   0   0   1\n",
    "# 1   3   4   0   0   1\n",
    "# 1   5   5   0   1   0     # B\n",
    "# 1   7   5   0   1   0\n",
    "# 1   2   5   0   1   0\n",
    "# 1   6   6   1   0   0     # A\n",
    "# 1   7   7   1   0   0\n",
    "\n",
    "xy = np.loadtxt('05train.txt', unpack=True, dtype='float32')\n",
    "\n",
    "# xy는 6x8. xy[:3]은 3x8. 행렬 곱셈을 하기 위해 미리 transpose.\n",
    "x_data = np.transpose(xy[:3])\n",
    "y_data = np.transpose(xy[3:])\n",
    "\n",
    "# print('x_data :', x_data.shape)     # x_data : (8, 3)\n",
    "# print('y_data :', y_data.shape)     # y_data : (8, 3)\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])  # x_data와 같은 크기의 열 가짐. 행 크기는 모름.\n",
    "Y = tf.placeholder(\"float\", [None, 3])  # tf.float32라고 써도 됨\n",
    "\n",
    "W = tf.Variable(tf.zeros([3, 3]))       # 3x3 행렬. 전체 0.\n",
    "\n",
    "# softmax 알고리즘 적용. X*W = (8x3) * (3x3) = (8x3)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W))\n",
    "\n",
    "# cross-entropy cost 함수\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), reduction_indices=1))\n",
    "\n",
    "learning_rate = 0.1\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(3001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            feed = {X: x_data, Y: y_data}\n",
    "            print('{:4} {:8.6}'.format(step, sess.run(cost, feed_dict=feed)), *sess.run(W))\n",
    "\n",
    "    print('-------------------------------')\n",
    "\n",
    "    # 1은 bias로 항상 1. (11, 7)은 x 입력\n",
    "    a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7]]})\n",
    "    print(\"a :\", a, sess.run(tf.argmax(a, 1)))         \n",
    "\n",
    "    b = sess.run(hypothesis, feed_dict={X: [[1, 5, 5]]})\n",
    "    print(\"b :\", b, sess.run(tf.argmax(b, 1)))         \n",
    "\n",
    "    c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0]]})\n",
    "    print(\"c :\", c, sess.run(tf.argmax(c, 1)))        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Human's Brain\n",
    "사람의 뇌와 비슷하게 동작하도록 구성. 일정 크기 이하라면 활성화(activation)되지 않도록 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](deep2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](deep3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](xor.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does Not Work.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.745717 [[ 0.86157721  0.12977192]]\n",
      "200 0.693876 [[ 0.11783948 -0.09426147]]\n",
      "400 0.693205 [[ 0.03054625 -0.03001043]]\n",
      "600 0.693152 [[ 0.00864731 -0.00863515]]\n",
      "800 0.693148 [[ 0.00246622 -0.00246592]]\n",
      "--------------------------------------------------\n",
      "[[ 0.5         0.49982291  0.50017709  0.5       ]]\n",
      "[[ 1.  0.  1.  1.]]\n",
      "[[False False  True False]]\n",
      "0.25\n",
      "Accuracy : 0.25\n"
     ]
    }
   ],
   "source": [
    "# 07train.txt\n",
    "# # x1 x2 y\n",
    "# 0   0   0\n",
    "# 0   1   1\n",
    "# 1   0   1\n",
    "# 1   1   0\n",
    "\n",
    "xy = np.loadtxt('07train.txt', unpack=True)\n",
    "\n",
    "x_data = xy[:-1]\n",
    "y_data = xy[-1]\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1, len(x_data)], -1.0, 1.0))\n",
    "\n",
    "h = tf.matmul(W, X)\n",
    "hypothesis = tf.div(1., 1. + tf.exp(-h))\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "\n",
    "rate = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1000):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    "    print('-'*50)\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.floor(hypothesis+0.5), Y)\n",
    "\n",
    "    #Calculate accuraty\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    param = [hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy]\n",
    "    result = sess.run(param, feed_dict={X:x_data, Y:y_data})\n",
    "\n",
    "    for i in result:\n",
    "        print(i)\n",
    "    print('Accuracy :', accuracy.eval({X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN for XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000 0.69318557 [[ 0.00879412  0.76684374 -0.30098861 -0.71702021]] [[ 0.65671682  0.14016879]]\n",
      " 3000 0.69141364 [[ 0.00748227  0.74723804 -0.16658312 -0.98316383]] [[ 0.57826585  0.40016973]]\n",
      " 5000 0.59383595 [[-0.4357602   2.54660511  0.37541351 -3.01620698]] [[ 0.65475595  2.58294272]]\n",
      " 7000 0.10400865 [[-4.22204494  5.10216236  3.87335396 -5.27188015]] [[ 5.89516592  6.19796276]]\n",
      " 9000 0.03573160 [[-5.28727436  5.78857756  4.96164322 -5.93484592]] [[ 8.04621792  8.01526165]]\n",
      "--------------------------------------------------\n",
      "[ 0.03074165] [ 0.97514027] [ 0.97754288] [ 0.02631707]\n",
      "[ 0.] [ 1.] [ 1.] [ 0.]\n",
      "[ True] [ True] [ True] [ True]\n",
      "1.0\n",
      "Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('07train.txt', unpack=True)\n",
    "\n",
    "x_data = np.transpose(xy[:-1])\n",
    "y_data = np.reshape(xy[-1], (4, 1))\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform([2, 2], -1.0, 1.0))\n",
    "W2 = tf.Variable(tf.random_uniform([2, 1], -1.0, 1.0))\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([2]))\n",
    "b2 = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "L2 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "hypothesis = tf.sigmoid(tf.matmul(L2, W2) + b2)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "\n",
    "rate = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(10000):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 2000 == 999:\n",
    "            # b1과 b2는 출력 생략. 한 줄에 출력하기 위해 reshape 사용\n",
    "            r1, (r2, r3) = sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run([W1, W2])\n",
    "            print('{:5} {:10.8f} {} {}'.format(step+1, r1, np.reshape(r2, (1,4)), np.reshape(r3, (1,2))))\n",
    "    print('-'*50)\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.floor(hypothesis+0.5), Y)\n",
    "\n",
    "    #Calculate accuraty\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    param = [hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy]\n",
    "    result = sess.run(param, feed_dict={X:x_data, Y:y_data})\n",
    "\n",
    "    print(*result[0])\n",
    "    print(*result[1])\n",
    "    print(*result[2])\n",
    "    print( result[-1])\n",
    "    print('Accuracy :', accuracy.eval({X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost= 0.548379772\n",
      "Epoch: 0002 cost= 0.366397427\n",
      "Epoch: 0003 cost= 0.336682462\n",
      "Epoch: 0004 cost= 0.321336086\n",
      "Epoch: 0005 cost= 0.311170549\n",
      "Epoch: 0006 cost= 0.304181358\n",
      "Epoch: 0007 cost= 0.298735990\n",
      "Epoch: 0008 cost= 0.294510695\n",
      "Epoch: 0009 cost= 0.290705519\n",
      "Epoch: 0010 cost= 0.287806567\n",
      "Epoch: 0011 cost= 0.285113236\n",
      "Epoch: 0012 cost= 0.282674599\n",
      "Epoch: 0013 cost= 0.280615871\n",
      "Epoch: 0014 cost= 0.278847154\n",
      "Epoch: 0015 cost= 0.277132416\n",
      "Epoch: 0016 cost= 0.275719005\n",
      "Epoch: 0017 cost= 0.274308553\n",
      "Epoch: 0018 cost= 0.273082635\n",
      "Epoch: 0019 cost= 0.271611717\n",
      "Epoch: 0020 cost= 0.270619700\n",
      "Epoch: 0021 cost= 0.269657017\n",
      "Epoch: 0022 cost= 0.268757150\n",
      "Epoch: 0023 cost= 0.267626746\n",
      "Epoch: 0024 cost= 0.266804289\n",
      "Epoch: 0025 cost= 0.266070729\n",
      "Label :  [4]\n",
      "Prediction : [4]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngaQIP5DU/wERhMk6uZfzZQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a6e8c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "Accuracy: 0.9237\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# Parameters.\n",
    "learning_rate = 0.1\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10])  # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Construct model\n",
    "hypothesis = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(hypothesis), reduction_indices=1))\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        # 나누어 떨어지지 않으면, 뒤쪽 이미지 일부는 사용하지 않는다.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs, y: batch_ys})\n",
    "\n",
    "            # 분할해서 구동하기 때문에 cost를 계속해서 누적시킨다. 전체 중의 일부에 대한 비용.\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step. display_step이 1이기 때문에 if는 필요없다.\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    # Label과 Prediction이 같은 값을 출력하면 맞는 것이다.\n",
    "    import random\n",
    "    r = random.randrange(mnist.test.num_examples)\n",
    "    print('Label : ', sess.run(tf.argmax(mnist.test.labels[r:r+1], 1)))\n",
    "    print('Prediction :', sess.run(tf.argmax(hypothesis, 1), {x: mnist.test.images[r:r+1]}))\n",
    "\n",
    "    # 1줄로 된 것을 28x28로 변환\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost= 172.728950706\n",
      "Epoch: 0002 cost= 45.692140680\n",
      "Epoch: 0003 cost= 28.759987365\n",
      "Epoch: 0004 cost= 20.081934328\n",
      "Epoch: 0005 cost= 14.611708541\n",
      "Epoch: 0006 cost= 10.837505576\n",
      "Epoch: 0007 cost= 8.093925250\n",
      "Epoch: 0008 cost= 5.996886490\n",
      "Epoch: 0009 cost= 4.451686376\n",
      "Epoch: 0010 cost= 3.238445550\n",
      "Epoch: 0011 cost= 2.456321038\n",
      "Epoch: 0012 cost= 1.827307447\n",
      "Epoch: 0013 cost= 1.462821515\n",
      "Epoch: 0014 cost= 1.200626686\n",
      "Epoch: 0015 cost= 0.817917825\n",
      "Label :  [9]\n",
      "Prediction : [9]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADWhJREFUeJzt3X+oXPWZx/HPo2lFkgi6mb0Gm+ztFWlQYRMZg1JZsmYT\nrBSvBZHkj3IXpKkYQwshGtI/1n9EXWyLolRuN9fGpSZZSCURpUXDqhSWklFjNHVdf3BrEq65E1Ko\nDUo26bN/zEn3Nt75zmTOmTknfd4vuNyZ85wz5+FwP/fMzHfmfM3dBSCeC8puAEA5CD8QFOEHgiL8\nQFCEHwiK8ANBEX4gKMIPBEX4gaDmDHJnCxYs8OHh4UHuEghlcnJSx44ds27WzRV+M7tF0mOSLpT0\nb+7+cGr94eFhNRqNPLsEkFCv17tet+en/WZ2oaQnJX1D0tWS1prZ1b0+HoDByvOaf7mkD9z9I3c/\nKWmHpNFi2gLQb3nCf4WkQzPuH86W/QUzW2dmDTNrNJvNHLsDUKS+v9vv7uPuXnf3eq1W6/fuAHQp\nT/iPSFo04/5XsmUAzgN5wr9P0lVm9lUz+7KkNZL2FNMWgH7reajP3U+Z2b2SfqXWUN+Eux8srDMA\nfZVrnN/dX5T0YkG9ABggPt4LBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBTh\nB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU\n4QeCIvxAULlm6TWzSUmfSjot6ZS714toCkD/5Qp/5h/d/VgBjwNggHjaDwSVN/wu6WUze93M1hXR\nEIDByPu0/yZ3P2JmfyvpJTP7b3d/beYK2T+FdZK0ePHinLsDUJRcZ353P5L9npb0nKTls6wz7u51\nd6/XarU8uwNQoJ7Db2ZzzWz+mduSVkt6p6jGAPRXnqf9Q5KeM7Mzj/Osu/+ykK4A9F3P4Xf3jyT9\nfYG9oEcnT55sW9u7d29y23vuuSdZn5ycTNazf/5tNRqNtrXrrrsuuS36i6E+ICjCDwRF+IGgCD8Q\nFOEHgiL8QFBFfKsPObl7sv7CCy8k61u2bGlbO3jwYE89nXHBBfnODzfeeGPb2ltvvZXcdsmSJbn2\njTTO/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOP8A3DixIlk/b777kvWn3rqqZ73fckllyTr119/\nfbK+bNmyZP3yyy9P1jdt2tS29t577yW3ZZy/vzjzA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQjPMP\nwJNPPpms5xnHl6Qbbrihbe3RRx9Nbpv6vn0RNm/e3Lb2+OOPJ7dduXJlsj5v3ryeekILZ34gKMIP\nBEX4gaAIPxAU4QeCIvxAUIQfCKrjOL+ZTUj6pqRpd782W3aZpJ2ShiVNSrrT3X/fvzarrdM0108/\n/XSux7/tttuS9WeffbZt7eKLL86177xS3+d/6KGHkttu27YtWV+/fn1PPaGlmzP/zyTdctayzZL2\nuvtVkvZm9wGcRzqG391fk3T8rMWjks78W94m6faC+wLQZ72+5h9y96ns9ieShgrqB8CA5H7Dz1sT\nzbWdbM7M1plZw8wazWYz7+4AFKTX8B81s4WSlP2ebreiu4+7e93d67VarcfdAShar+HfI2ksuz0m\naXcx7QAYlI7hN7Ptkv5L0tfM7LCZ3SXpYUmrzOx9Sf+U3QdwHuk4zu/ua9uU0l+2DuTAgQPJ+smT\nJ5P11atXJ+s7duxI1i+66KJkvUydruufsnPnzmSdcf58+IQfEBThB4Ii/EBQhB8IivADQRF+ICgu\n3d2l48fP/m7T//v4449zPfYjjzySrFd5KO/DDz9M1u+///4BdYJzxZkfCIrwA0ERfiAowg8ERfiB\noAg/EBThB4JinL9L09NtL1ako0eP5nrsJUuW5No+5fTp08n6Z599lqxv3749Wd+4cWOy/vnnnyfr\nKA9nfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IinH+LqXG4kdHR5Pb7tq1K1lPTWMtSStXpq+S/uqr\nr7atdfoMQqdxfPz14swPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0F1HOc3swlJ35Q07e7XZssekPQd\nSc1stS3u/mK/mqy6rVu3Jus333xzsr5hw4Zk/YknnjjnngZl8eLFyfqhQ4fa1ty96HZwDro58/9M\n0i2zLP+xuy/NfsIGHzhfdQy/u78mqf10NQDOS3le828wswNmNmFmlxbWEYCB6DX8P5E0ImmppClJ\nP2y3opmtM7OGmTWazWa71QAMWE/hd/ej7n7a3f8k6aeSlifWHXf3urvXa7Var30CKFhP4TezhTPu\nfkvSO8W0A2BQuhnq2y5phaQFZnZY0r9IWmFmSyW5pElJ3+1jjwD6oGP43X3tLIvTA9vBzJ8/P1m/\n++67k/VVq1Yl6w8++OA591SUe++9N1kfGRlJ1oeGhtrWTp06ldz2mmuuSdaRD5/wA4Ii/EBQhB8I\nivADQRF+ICjCDwTFpbsr4Morr0zWJyYmBtRJtRw/zvfJ+okzPxAU4QeCIvxAUIQfCIrwA0ERfiAo\nwg8ExTg/KmtqaqrsFv6qceYHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAY50cuu3fvTtY7XZ47ZcWK\nFT1vi8448wNBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUB3H+c1skaRnJA1Jcknj7v6YmV0maaekYUmT\nku5099/3r1VU0ejoaLI+Z077P7FOnwF45ZVXemkJXermzH9K0kZ3v1rSDZLWm9nVkjZL2uvuV0na\nm90HcJ7oGH53n3L3N7Lbn0p6V9IVkkYlbctW2ybp9n41CaB45/Sa38yGJS2T9BtJQ+5+5jpLn6j1\nsgDAeaLr8JvZPEm7JH3f3f8ws+burtb7AbNtt87MGmbWaDabuZoFUJyuwm9mX1Ir+D93919ki4+a\n2cKsvlDS9Gzbuvu4u9fdvV6r1YroGUABOobfzEzSVknvuvuPZpT2SBrLbo9JSn+9C0CldPOV3q9L\n+rakt81sf7Zsi6SHJf2Hmd0l6XeS7uxPi6iyEydO9O2x16xZ07fHRhfhd/dfS7I25ZXFtgNgUPiE\nHxAU4QeCIvxAUIQfCIrwA0ERfiAoLt2NXJ5//vlkPc+lu+fOndvztuiMMz8QFOEHgiL8QFCEHwiK\n8ANBEX4gKMIPBMU4Pypr3759yfrY2FiyjjTO/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOP8qCyu\n299fnPmBoAg/EBThB4Ii/EBQhB8IivADQRF+IKiO4/xmtkjSM5KGJLmkcXd/zMwekPQdSc1s1S3u\n/mK/GkU13XHHHcn6oUOH2tbefPPN5LYjIyM99YTudPMhn1OSNrr7G2Y2X9LrZvZSVvuxuz/av/YA\n9EvH8Lv7lKSp7PanZvaupCv63RiA/jqn1/xmNixpmaTfZIs2mNkBM5sws0vbbLPOzBpm1mg2m7Ot\nAqAEXYffzOZJ2iXp++7+B0k/kTQiaalazwx+ONt27j7u7nV3r9dqtQJaBlCErsJvZl9SK/g/d/df\nSJK7H3X30+7+J0k/lbS8f20CKFrH8JuZSdoq6V13/9GM5QtnrPYtSe8U3x6Afunm3f6vS/q2pLfN\nbH+2bIuktWa2VK3hv0lJ3+1Lh6i0OXPSf0KbNm0aUCc4V9282/9rSTZLiTF94DzGJ/yAoAg/EBTh\nB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBmbsPbmdmTUm/m7FogaRj\nA2vg3FS1t6r2JdFbr4rs7e/cvavr5Q00/F/YuVnD3eulNZBQ1d6q2pdEb70qqzee9gNBEX4gqLLD\nP17y/lOq2ltV+5LorVel9Fbqa34A5Sn7zA+gJKWE38xuMbP3zOwDM9tcRg/tmNmkmb1tZvvNrFFy\nLxNmNm1m78xYdpmZvWRm72e/Z50mraTeHjCzI9mx229mt5bU2yIz+08z+62ZHTSz72XLSz12ib5K\nOW4Df9pvZhdK+h9JqyQdlrRP0lp3/+1AG2nDzCYl1d299DFhM/sHSX+U9Iy7X5st+1dJx9394ewf\n56Xufn9FentA0h/Lnrk5m1Bm4cyZpSXdLumfVeKxS/R1p0o4bmWc+ZdL+sDdP3L3k5J2SBotoY/K\nc/fXJB0/a/GopG3Z7W1q/fEMXJveKsHdp9z9jez2p5LOzCxd6rFL9FWKMsJ/haRDM+4fVrWm/HZJ\nL5vZ62a2ruxmZjGUTZsuSZ9IGiqzmVl0nLl5kM6aWboyx66XGa+Lxht+X3STuy+V9A1J67Ont5Xk\nrddsVRqu6Wrm5kGZZWbpPyvz2PU643XRygj/EUmLZtz/SrasEtz9SPZ7WtJzqt7sw0fPTJKa/Z4u\nuZ8/q9LMzbPNLK0KHLsqzXhdRvj3SbrKzL5qZl+WtEbSnhL6+AIzm5u9ESMzmytptao3+/AeSWPZ\n7TFJu0vs5S9UZebmdjNLq+RjV7kZr9194D+SblXrHf8PJf2gjB7a9DUi6a3s52DZvUnartbTwP9V\n672RuyT9jaS9kt6X9LKkyyrU279LelvSAbWCtrCk3m5S6yn9AUn7s59byz52ib5KOW58wg8Iijf8\ngKAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8E9X+0Fx0fuhfbRgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12b073fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "Accuracy: 0.9419\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# Parameters. 반복문에서 사용하는데, 미리 만들어 놓았다.\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph Input\n",
    "X = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "Y = tf.placeholder(tf.float32, [None, 10])  # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "W3 = tf.Variable(tf.random_normal([256,  10]))\n",
    "\n",
    "B1 = tf.Variable(tf.random_normal([256]))\n",
    "B2 = tf.Variable(tf.random_normal([256]))\n",
    "B3 = tf.Variable(tf.random_normal([ 10]))\n",
    "\n",
    "# Construct model\n",
    "L1 = tf.nn.relu(tf.add(tf.matmul(X, W1), B1))\n",
    "L2 = tf.nn.relu(tf.add(tf.matmul(L1, W2), B2)) # Hidden layer with ReLU activation\n",
    "hypothesis = tf.add(tf.matmul(L2, W3), B3)     # No need to use softmax here\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))   # softmax loss\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        # 나누어 떨어지지 않으면, 뒤쪽 이미지 일부는 사용하지 않는다.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "\n",
    "            # 분할해서 구동하기 때문에 cost를 계속해서 누적시킨다. 전체 중의 일부에 대한 비용.\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step. display_step이 1이기 때문에 if는 필요없다.\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    # Label과 Prediction이 같은 값을 출력하면 맞는 것이다.\n",
    "    import random\n",
    "    r = random.randrange(mnist.test.num_examples)\n",
    "    print('Label : ', sess.run(tf.argmax(mnist.test.labels[r:r+1], 1)))\n",
    "    print('Prediction :', sess.run(tf.argmax(hypothesis, 1), {X: mnist.test.images[r:r+1]}))\n",
    "\n",
    "    # 1줄로 된 것을 28x28로 변환\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Next.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
