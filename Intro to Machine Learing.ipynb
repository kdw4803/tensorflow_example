{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Machine Learning (feat. Tensorflow)\n",
    "\n",
    "2017.4.13\n",
    "Dongwoo 화목회"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is the subfield of computer science that gives computers the ability to learn without being explicitly programmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised vs Unsupervised ( vs reinforcement )\n",
    "- supervised\n",
    "    - Regression problem\n",
    "    - Classification problem\n",
    "- unsupervised\n",
    "    - Clustering\n",
    "- reinforecement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow\n",
    "Tensorflow is an open-source software library for Machine Intelligence. It was developed by Google to meet their needs for systems capable of building and training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Start.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "H(x) = Wx + b에서 Wx + b는 x에 대한 1차 방적식으로 직선을 표현한다. 기울기에 해당하는 W(Weight)와 절편에 해당하는 b(bias)가 반복되는 과정에서 계속 바뀌고, 마지막 루프에서 바뀐 최종 값을 사용해서 데이터 예측(prediction)에 사용하게 된다. 최종 결과로 나온 가설을 모델(model)이라고 부르고, \"학습되었다\"라고 한다.\n",
    "![title](hypothesis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "앞에서 설명한 Hypothesis 방정식에 대한 비용(cost)으로 방정식의 결과가 크게 나오면 좋지 않다고 얘기하고, 루프를 돌 때마다 W와 b를 비용이 적게 발생하는 방향으로 수정하게 된다.\n",
    "![title](cost_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal : Minimize cost\n",
    "목표는 cost를 최소로 만드는 W(기울기)와 b(절편)를 찾는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "2차원 좌표에 분포된 데이터를 1차원 직선 방정식을 통해 표현되지 않은 데이터를 예측하기 위한 분석 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](linear1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](linear2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 206.674 [-9.96474075] [ 35.17779541]\n",
      "400 62.289 [-4.56849813] [ 19.31218338]\n",
      "600 18.7731 [-1.60602582] [ 10.60215569]\n",
      "800 5.65799 [ 0.02033605] [ 5.82045364]\n",
      "1000 1.70525 [ 0.91318834] [ 3.19535875]\n",
      "1200 0.513941 [ 1.40335345] [ 1.75421357]\n",
      "1400 0.154896 [ 1.67244816] [ 0.9630428]\n",
      "1600 0.0466837 [ 1.82017791] [ 0.52869904]\n",
      "1800 0.0140699 [ 1.90127993] [ 0.2902492]\n",
      "2000 0.0042405 [ 1.94580364] [ 0.15934351]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEKCAYAAAASByJ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCAHA/wfhvXbF2FGaLAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122b57b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_data = [1., 2., 3., 4.]\n",
    "y_data = [2., 4., 6., 8.]\n",
    "\n",
    "# range is -100 ~ 100\n",
    "W = tf.Variable(tf.random_uniform([1], -100., 100.))\n",
    "b = tf.Variable(tf.random_uniform([1], -100., 100.))\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "hypothesis = W * X + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "rate = tf.Variable(0.01)\n",
    "optimizer = tf.train.GradientDescentOptimizer(rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(2001):\n",
    "    sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 200 == 0 and step >= 200:\n",
    "        print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W), sess.run(b))\n",
    "        plt.plot(x_data,sess.run(W)*x_data + sess.run(b))\n",
    "\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x')\n",
    "plt.show()\n",
    "\n",
    "# print(sess.run(hypothesis, feed_dict={X: 5}))           # [ 10.]\n",
    "# print(sess.run(hypothesis, feed_dict={X: 2.5}))         # [5.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Classification\n",
    "Linear Regression을 활용해서 데이터를 분류하는 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid\n",
    "sigmoid는 linear regression에서 가져온 값을 0과 1 사이의 값으로 변환한다. x가 0일 때, 0.5가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New cost function\n",
    "sigmoid 함수 적용으로 인해 새로운 cost function를 사용해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](cost_function2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Decent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](gradient.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.837924 [[ 0.67223454 -0.10460093  0.3180154 ]]\n",
      "200 0.500997 [[-1.48802495  0.12843084  0.35273504]]\n",
      "400 0.423672 [[-2.7033565   0.24845743  0.51990491]]\n",
      "600 0.390119 [[-3.50705624  0.31958625  0.63729244]]\n",
      "800 0.372174 [[-4.09583426  0.3660709   0.72739005]]\n",
      "1000 0.361234 [[-4.55587435  0.39870661  0.80029243]]\n",
      "1200 0.35396 [[-4.93113947  0.4228183   0.86139172]]\n",
      "1400 0.348815 [[-5.24675035  0.44131786  0.91389936]]\n",
      "1600 0.345007 [[-5.51830196  0.45593175  0.95987999]]\n",
      "1800 0.342086 [[-5.75608969  0.46774739  1.0007385 ]]\n",
      "2000 0.339783 [[-5.96724463  0.47748369  1.03747296]]\n",
      "-----------------------------------------\n",
      "[1, 2, 1] : [[False]]\n",
      "[1, 5, 5] : [[ True]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 04train.txt\n",
    "# #x0 x1 x2 y\n",
    "# 1   2   1   0\n",
    "# 1   3   2   0\n",
    "# 1   3   5   0\n",
    "# 1   5   5   1\n",
    "# 1   7   5   1\n",
    "# 1   2   5   1\n",
    "\n",
    "# 원본 파일은 6행 4열이지만, 열 우선이라서 4행 6열로 가져옴\n",
    "xy = np.loadtxt('04train.txt', unpack=True, dtype='float32')\n",
    "\n",
    "# print(xy[0], xy[-1])        # [ 1.  1.  1.  1.  1.  1.] [ 0.  0.  0.  1.  1.  1.]\n",
    "\n",
    "x_data = xy[:-1]            # 3행 6열\n",
    "y_data = xy[-1]             # 1행 6열\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# feature별 가중치를 난수로 초기화. feature는 bias 포함해서 3개. 1행 3열.\n",
    "W = tf.Variable(tf.random_uniform([1, len(x_data)], -1.0, 1.0))\n",
    "\n",
    "# 행렬 곱셈. (1x3) * (3x6)\n",
    "h = tf.matmul(W, X)\n",
    "hypothesis = tf.div(1., 1. + tf.exp(-h))    # exp(-h) = e ** -h. e는 자연상수\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "\n",
    "rate = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(2001):\n",
    "    sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 200 == 0:\n",
    "        print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    "\n",
    "print('-----------------------------------------')\n",
    "\n",
    "# 결과가 0 또는 1로 계산되는 것이 아니라 0과 1 사이의 값으로 나오기 때문에 True/False는 직접 판단\n",
    "print('[1, 2, 1] :', sess.run(hypothesis, feed_dict={X: [[1], [2], [1]]}) > 0.5)\n",
    "print('[1, 5, 5] :', sess.run(hypothesis, feed_dict={X: [[1], [5], [5]]}) > 0.5)\n",
    "sess.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression\n",
    "Logistic Regression을 부르는 다른 이름은 binary classification이다. 데이터를 1과 0의 두 가지 그룹으로 나누기 위해 사용하는 모델이다. Softmax는 데이터를 2개 이상의 그룹으로 나누기 위해 binary classification을 확장한 모델이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](softmax1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "softmax는 점수로 나온 결과를 전체 합계가 1이 되는 0과 1 사이의 값으로 변경해 준다. 전체를 더하면 1이 되기 때문에 확률(probabilites)이라고 부르면 의미가 더욱 분명해진다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](softmax2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "one-hot encoding은 softmax로 구한 값 중에서 가장 큰 값을 1로, 나머지를 0으로 만든다. 어떤 것을 선택할지를 확실하게 정리해 준다. one-hot encoding은 설명한 것처럼 매우 간단하기 때문에 직접 구현할 수도 있지만, 텐서플로우에서는 argmax 함수라는 이름으로 제공하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](softmax3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy Cost Function\n",
    "S(Y)는 softmax가 예측한 값이고, L(Y)는 실제 Y의 값으로 L은 label을 의미한다. cost 함수는 예측한 값과 실제 값의 거리(distance, D)를 계산하는 함수로, 이 값이 줄어드는 방향으로, 즉 entropy가 감소하는 방향으로 진행하다 보면 최저점을 만나게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](softmax4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](softmax5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1.0678 [-0.00833333  0.00416667  0.00416666] [ 0.01666667  0.02916667 -0.04583334] [ 0.01666666  0.04166667 -0.05833334]\n",
      " 200 0.699681 [-1.5737735  -0.36410642  1.93788099] [ 0.0967759  -0.09235884 -0.00441682] [ 0.24915566  0.23823613 -0.48739177]\n",
      " 400 0.594579 [-2.54309034 -0.43173078  2.97482228] [ 0.13064831 -0.04841127 -0.08223619] [ 0.40734777  0.22857714 -0.63592446]\n",
      " 600 0.535829 [-3.31521416 -0.39115471  3.70637012] [ 0.13982886 -0.02464214 -0.11518568] [ 0.54843497  0.21297167 -0.76140594]\n",
      " 800 0.494312 [-3.98393679 -0.31143472  4.29537106] [ 0.1417881  -0.00935038 -0.13243635] [ 0.6748786   0.1950776  -0.86995488]\n",
      "1000 0.461916 [-4.58334923 -0.21852757  4.8018775 ] [ 0.14145668  0.00154199 -0.14299677] [ 0.78935486  0.17700934 -0.9663626 ]\n",
      "1200 0.435343 [-5.13021469 -0.12343737  5.25365353] [ 0.14044461  0.00979479 -0.15023731] [ 0.89399701  0.15975396 -1.05374885]\n",
      "1400 0.412891 [-5.63466215 -0.03106253  5.66572714] [ 0.1393245   0.01630212 -0.15562418] [ 0.99044132  0.14372151 -1.13416028]\n",
      "1600 0.393535 [-6.10366583  0.05645988  6.04720736] [ 0.13829574  0.02157287 -0.15986556] [ 1.07996321  0.12904176 -1.20900333]\n",
      "1800 0.376595 [-6.54242659  0.13831529  6.40411282] [ 0.13741526  0.02592302 -0.16333508] [ 1.16357517  0.11571171 -1.27928567]\n",
      "2000 0.361591 [-6.95501471  0.21433842  6.7406764 ] [ 0.13668649  0.02956286 -0.16624542] [ 1.2420913  0.1036661 -1.3457557]\n",
      "2200 0.348169 [-7.34470749  0.28468168  7.06002617] [ 0.13609275  0.03263915 -0.16872707] [ 1.31617439  0.09281436 -1.40898621]\n",
      "2400  0.33606 [-7.71420002  0.34964585  7.36455774] [ 0.13561255  0.03525931 -0.17086639] [ 1.38636887  0.08305811 -1.46942413]\n",
      "2600 0.325053 [-8.06574821  0.40959236  7.65615749] [ 0.13522536  0.03750427 -0.17272341] [ 1.45312846  0.07430083 -1.52742565]\n",
      "2800 0.314984 [-8.40124512  0.46489772  7.93635178] [ 0.13491319  0.03943677 -0.17434347] [ 1.51683164  0.06645141 -1.58327949]\n",
      "3000 0.305719 [-8.72231579  0.51593047  8.20638561] [ 0.13466108  0.04110633 -0.17576036] [ 1.57780051  0.0594258  -1.63722253]\n",
      "-------------------------------\n",
      "a : [[  9.18195665e-01   8.16900209e-02   1.14327857e-04]] [0]\n",
      "b : [[ 0.21065326  0.68458456  0.10476214]] [1]\n",
      "c : [[  6.06074408e-08   5.67564450e-04   9.99432385e-01]] [2]\n"
     ]
    }
   ],
   "source": [
    "# softmax이기 때문에 y를 표현할 때, 벡터로 표현한다.\n",
    "# 1개의 값으로 표현한다고 할 때, 뭐라고 쓸지도 사실 애매하다.\n",
    "\n",
    "# 05train.txt\n",
    "# #x0 x1 x2 y[A   B   C]\n",
    "# 1   2   1   0   0   1     # C\n",
    "# 1   3   2   0   0   1\n",
    "# 1   3   4   0   0   1\n",
    "# 1   5   5   0   1   0     # B\n",
    "# 1   7   5   0   1   0\n",
    "# 1   2   5   0   1   0\n",
    "# 1   6   6   1   0   0     # A\n",
    "# 1   7   7   1   0   0\n",
    "\n",
    "xy = np.loadtxt('05train.txt', unpack=True, dtype='float32')\n",
    "\n",
    "# xy는 6x8. xy[:3]은 3x8. 행렬 곱셈을 하기 위해 미리 transpose.\n",
    "x_data = np.transpose(xy[:3])\n",
    "y_data = np.transpose(xy[3:])\n",
    "\n",
    "# print('x_data :', x_data.shape)     # x_data : (8, 3)\n",
    "# print('y_data :', y_data.shape)     # y_data : (8, 3)\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])  # x_data와 같은 크기의 열 가짐. 행 크기는 모름.\n",
    "Y = tf.placeholder(\"float\", [None, 3])  # tf.float32라고 써도 됨\n",
    "\n",
    "W = tf.Variable(tf.zeros([3, 3]))       # 3x3 행렬. 전체 0.\n",
    "\n",
    "# softmax 알고리즘 적용. X*W = (8x3) * (3x3) = (8x3)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W))\n",
    "\n",
    "# cross-entropy cost 함수\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), reduction_indices=1))\n",
    "\n",
    "learning_rate = 0.1\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(3001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            feed = {X: x_data, Y: y_data}\n",
    "            print('{:4} {:8.6}'.format(step, sess.run(cost, feed_dict=feed)), *sess.run(W))\n",
    "\n",
    "    print('-------------------------------')\n",
    "\n",
    "    # 1은 bias로 항상 1. (11, 7)은 x 입력\n",
    "    a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7]]})\n",
    "    print(\"a :\", a, sess.run(tf.argmax(a, 1)))         \n",
    "\n",
    "    b = sess.run(hypothesis, feed_dict={X: [[1, 5, 5]]})\n",
    "    print(\"b :\", b, sess.run(tf.argmax(b, 1)))         \n",
    "\n",
    "    c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0]]})\n",
    "    print(\"c :\", c, sess.run(tf.argmax(c, 1)))        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Human's Brain\n",
    "사람의 뇌와 비슷하게 동작하도록 구성. 일정 크기 이하라면 활성화(activation)되지 않도록 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](deep2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](deep3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](xor.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does Not Work.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.745717 [[ 0.86157721  0.12977192]]\n",
      "200 0.693876 [[ 0.11783948 -0.09426147]]\n",
      "400 0.693205 [[ 0.03054625 -0.03001043]]\n",
      "600 0.693152 [[ 0.00864731 -0.00863515]]\n",
      "800 0.693148 [[ 0.00246622 -0.00246592]]\n",
      "--------------------------------------------------\n",
      "[[ 0.5         0.49982291  0.50017709  0.5       ]]\n",
      "[[ 1.  0.  1.  1.]]\n",
      "[[False False  True False]]\n",
      "0.25\n",
      "Accuracy : 0.25\n"
     ]
    }
   ],
   "source": [
    "# 07train.txt\n",
    "# # x1 x2 y\n",
    "# 0   0   0\n",
    "# 0   1   1\n",
    "# 1   0   1\n",
    "# 1   1   0\n",
    "\n",
    "xy = np.loadtxt('07train.txt', unpack=True)\n",
    "\n",
    "x_data = xy[:-1]\n",
    "y_data = xy[-1]\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1, len(x_data)], -1.0, 1.0))\n",
    "\n",
    "h = tf.matmul(W, X)\n",
    "hypothesis = tf.div(1., 1. + tf.exp(-h))\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "\n",
    "rate = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1000):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    "    print('-'*50)\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.floor(hypothesis+0.5), Y)\n",
    "\n",
    "    #Calculate accuraty\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    param = [hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy]\n",
    "    result = sess.run(param, feed_dict={X:x_data, Y:y_data})\n",
    "\n",
    "    for i in result:\n",
    "        print(i)\n",
    "    print('Accuracy :', accuracy.eval({X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN for XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000 0.69318557 [[ 0.00879412  0.76684374 -0.30098861 -0.71702021]] [[ 0.65671682  0.14016879]]\n",
      " 3000 0.69141364 [[ 0.00748227  0.74723804 -0.16658312 -0.98316383]] [[ 0.57826585  0.40016973]]\n",
      " 5000 0.59383595 [[-0.4357602   2.54660511  0.37541351 -3.01620698]] [[ 0.65475595  2.58294272]]\n",
      " 7000 0.10400865 [[-4.22204494  5.10216236  3.87335396 -5.27188015]] [[ 5.89516592  6.19796276]]\n",
      " 9000 0.03573160 [[-5.28727436  5.78857756  4.96164322 -5.93484592]] [[ 8.04621792  8.01526165]]\n",
      "--------------------------------------------------\n",
      "[ 0.03074165] [ 0.97514027] [ 0.97754288] [ 0.02631707]\n",
      "[ 0.] [ 1.] [ 1.] [ 0.]\n",
      "[ True] [ True] [ True] [ True]\n",
      "1.0\n",
      "Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('07train.txt', unpack=True)\n",
    "\n",
    "x_data = np.transpose(xy[:-1])\n",
    "y_data = np.reshape(xy[-1], (4, 1))\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform([2, 2], -1.0, 1.0))\n",
    "W2 = tf.Variable(tf.random_uniform([2, 1], -1.0, 1.0))\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([2]))\n",
    "b2 = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "L2 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "hypothesis = tf.sigmoid(tf.matmul(L2, W2) + b2)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "\n",
    "rate = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(10000):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 2000 == 999:\n",
    "            # b1과 b2는 출력 생략. 한 줄에 출력하기 위해 reshape 사용\n",
    "            r1, (r2, r3) = sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run([W1, W2])\n",
    "            print('{:5} {:10.8f} {} {}'.format(step+1, r1, np.reshape(r2, (1,4)), np.reshape(r3, (1,2))))\n",
    "    print('-'*50)\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.floor(hypothesis+0.5), Y)\n",
    "\n",
    "    #Calculate accuraty\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    param = [hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy]\n",
    "    result = sess.run(param, feed_dict={X:x_data, Y:y_data})\n",
    "\n",
    "    print(*result[0])\n",
    "    print(*result[1])\n",
    "    print(*result[2])\n",
    "    print( result[-1])\n",
    "    print('Accuracy :', accuracy.eval({X:x_data, Y:y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost= 0.548379772\n",
      "Epoch: 0002 cost= 0.366397427\n",
      "Epoch: 0003 cost= 0.336682462\n",
      "Epoch: 0004 cost= 0.321336086\n",
      "Epoch: 0005 cost= 0.311170549\n",
      "Epoch: 0006 cost= 0.304181358\n",
      "Epoch: 0007 cost= 0.298735990\n",
      "Epoch: 0008 cost= 0.294510695\n",
      "Epoch: 0009 cost= 0.290705519\n",
      "Epoch: 0010 cost= 0.287806567\n",
      "Epoch: 0011 cost= 0.285113236\n",
      "Epoch: 0012 cost= 0.282674599\n",
      "Epoch: 0013 cost= 0.280615871\n",
      "Epoch: 0014 cost= 0.278847154\n",
      "Epoch: 0015 cost= 0.277132416\n",
      "Epoch: 0016 cost= 0.275719005\n",
      "Epoch: 0017 cost= 0.274308553\n",
      "Epoch: 0018 cost= 0.273082635\n",
      "Epoch: 0019 cost= 0.271611717\n",
      "Epoch: 0020 cost= 0.270619700\n",
      "Epoch: 0021 cost= 0.269657017\n",
      "Epoch: 0022 cost= 0.268757150\n",
      "Epoch: 0023 cost= 0.267626746\n",
      "Epoch: 0024 cost= 0.266804289\n",
      "Epoch: 0025 cost= 0.266070729\n",
      "Label :  [4]\n",
      "Prediction : [4]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngaQIP5DU/wERhMk6uZfzZQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a6e8c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "Accuracy: 0.9237\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# Parameters.\n",
    "learning_rate = 0.1\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10])  # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Construct model\n",
    "hypothesis = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(hypothesis), reduction_indices=1))\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        # 나누어 떨어지지 않으면, 뒤쪽 이미지 일부는 사용하지 않는다.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs, y: batch_ys})\n",
    "\n",
    "            # 분할해서 구동하기 때문에 cost를 계속해서 누적시킨다. 전체 중의 일부에 대한 비용.\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step. display_step이 1이기 때문에 if는 필요없다.\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    # Label과 Prediction이 같은 값을 출력하면 맞는 것이다.\n",
    "    import random\n",
    "    r = random.randrange(mnist.test.num_examples)\n",
    "    print('Label : ', sess.run(tf.argmax(mnist.test.labels[r:r+1], 1)))\n",
    "    print('Prediction :', sess.run(tf.argmax(hypothesis, 1), {x: mnist.test.images[r:r+1]}))\n",
    "\n",
    "    # 1줄로 된 것을 28x28로 변환\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost= 0.194413211\n",
      "Epoch: 0002 cost= 0.058463944\n",
      "Epoch: 0003 cost= 0.039161217\n",
      "Epoch: 0004 cost= 0.029039243\n",
      "Epoch: 0005 cost= 0.022257142\n",
      "Epoch: 0006 cost= 0.017475904\n",
      "Epoch: 0007 cost= 0.013757427\n",
      "Epoch: 0008 cost= 0.010988224\n",
      "Epoch: 0009 cost= 0.008736229\n",
      "Epoch: 0010 cost= 0.007020326\n",
      "Epoch: 0011 cost= 0.005604672\n",
      "Epoch: 0012 cost= 0.004423175\n",
      "Epoch: 0013 cost= 0.003485011\n",
      "Epoch: 0014 cost= 0.002778560\n",
      "Epoch: 0015 cost= 0.002194124\n",
      "Epoch: 0016 cost= 0.001659622\n",
      "Epoch: 0017 cost= 0.001290463\n",
      "Epoch: 0018 cost= 0.001008104\n",
      "Epoch: 0019 cost= 0.000801290\n",
      "Epoch: 0020 cost= 0.000634146\n",
      "Epoch: 0021 cost= 0.000478845\n",
      "Epoch: 0022 cost= 0.000358009\n",
      "Epoch: 0023 cost= 0.000297571\n",
      "Epoch: 0024 cost= 0.000242255\n",
      "Epoch: 0025 cost= 0.000189631\n",
      "Label :  [0]\n",
      "Prediction : [0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a0bc128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Finished!\n",
      "Accuracy: 0.9497\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# Parameters. 반복문에서 사용하는데, 미리 만들어 놓았다.\n",
    "learning_rate = 0.001\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph Input\n",
    "X = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "Y = tf.placeholder(tf.float32, [None, 10])  # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "W3 = tf.Variable(tf.random_normal([256,  10]))\n",
    "\n",
    "B1 = tf.Variable(tf.random_normal([256]))\n",
    "B2 = tf.Variable(tf.random_normal([256]))\n",
    "B3 = tf.Variable(tf.random_normal([ 10]))\n",
    "\n",
    "# Construct model\n",
    "L1 = tf.sigmoid(tf.add(tf.matmul(X, W1), B1))\n",
    "L2 = tf.sigmoid(tf.add(tf.matmul(L1, W2), B2))\n",
    "\n",
    "# Construct model\n",
    "hypothesis = tf.nn.softmax(tf.matmul(L2, W3) + B3) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_mean(Y*tf.log(hypothesis), reduction_indices=1))\n",
    "\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        # 나누어 떨어지지 않으면, 뒤쪽 이미지 일부는 사용하지 않는다.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "\n",
    "            # 분할해서 구동하기 때문에 cost를 계속해서 누적시킨다. 전체 중의 일부에 대한 비용.\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step. display_step이 1이기 때문에 if는 필요없다.\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    # Label과 Prediction이 같은 값을 출력하면 맞는 것이다.\n",
    "    import random\n",
    "    r = random.randrange(mnist.test.num_examples)\n",
    "    print('Label : ', sess.run(tf.argmax(mnist.test.labels[r:r+1], 1)))\n",
    "    print('Prediction :', sess.run(tf.argmax(hypothesis, 1), {X: mnist.test.images[r:r+1]}))\n",
    "\n",
    "    # 1줄로 된 것을 28x28로 변환\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Next.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
